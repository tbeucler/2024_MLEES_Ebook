{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2024_MLEES_Ebook/blob/main/Milton/09_GenerativeAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Exercise) Introduction to Uncertainty Quantification and Generative Modeling\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/msgomez06/ML_pedagogical_materials/blob/main/images/09.01-missing.png?raw=True\" width=90%></img>\n",
        "\n",
        "We're often tasked with filling voids and to express how sure we are of our answer.\n",
        "<br> <i> How can we train an algorithm to do the same? </i>\n",
        "</center>"
      ],
      "metadata": {
        "id": "yxgxKxVNRDyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Prerequisite Imports\n",
        "#@markdown Run this cell for preliminary requirements. Double click it if you want to check out the source :)\n",
        "\n",
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# Import pooch - used to handle data downloading\n",
        "import pooch\n",
        "\n",
        "colors = [np.array([215, 166, 122])/255, np.array([0, 148, 199])/255, np.array([214, 7, 114])/255]"
      ],
      "metadata": {
        "id": "POzUOOb62D3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Quick Introduction to Uncertainty\n",
        "\n",
        "Uncertainty is one of those terms that you are likely very familiar with, whether in the colloquial sense (I'd have a hard time believing that any of us have *never* had a moment of doubt) or the scientific sense.\n",
        "\n",
        "Today, we'll be using <a href=\"https://doi.org/10.1175/AIES-D-22-0061.1\">Haynes et al.'s paper on uncertainty estimates with neural networks for environmental science applications</a> as a guide for our efforts. Let us then begin as they did - by discussing the different types of uncertainty we expect to encounter.\n",
        "\n",
        "Generally speaking, we expect to encounter two types of uncertainty - reducible and irreducible uncertainty.\n",
        "\n"
      ],
      "metadata": {
        "id": "v57XhsEJp-Ju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVMhM_V8Q8hw"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "# prompt: Write a small python script that illustrates how machine precision affects floating point operations\n",
        "\n",
        "a = 1.0\n",
        "b = 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1\n",
        "print(a == b)  # False due to floating-point precision limitations\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "c = 0.3\n",
        "d = 0.1 + 0.2\n",
        "print(c == d)  # Also False due to floating-point precision\n",
        "\n",
        "import math\n",
        "print(math.isclose(a, b))  # True, using a tolerance for comparison\n",
        "print(math.isclose(c, d))  # True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "# prompt: Make a namespace with \"with\" that allows me to only import numpy within that namespace\n",
        "\n",
        "with np_namespace():\n",
        "  import numpy as np\n",
        "  print(np.array([1, 2, 3]))\n",
        "\n",
        "def np_namespace():\n",
        "  class Namespace:\n",
        "    pass\n",
        "  return Namespace()\n"
      ],
      "metadata": {
        "id": "3VeymyNKtYNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation and Analysis\n",
        "First, let's download a dataset we prepared for this notebook and plot it. Like before, we'll rely on pooch to load the data from OneDrive, and"
      ],
      "metadata": {
        "id": "e4MbwINARR0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the files from the cloud using Pooch.\n",
        "data_url = 'https://unils-my.sharepoint.com/:u:/g/personal/tom_beucler_unil_ch/EUAqVt0ZDwNNlVTALKSu_foBBgaBoWZy1A2bxoVPCHBFfA?download=1'\n",
        "hash = '1ed5315572b919ed192e50ecaecff18a060db99d58119c151bbbe26028aa449c'\n",
        "\n",
        "files = pooch.retrieve(data_url, known_hash=hash, processor=pooch.Unzip())\n",
        "[print(f'File at index {idx}: {filename.split(\"/\")[-1]}') for idx, filename in enumerate(files)];\n",
        "\n",
        "# look for UQ_x in the filename to get the x file\n",
        "for filename in files:\n",
        "    if 'UQ_y' in filename:\n",
        "        y_file = filename\n",
        "    elif 'UQ_x' in filename:\n",
        "        x_file = filename"
      ],
      "metadata": {
        "id": "r5grRwXtOzE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the files. Note that they're npy files\n",
        "y = np.load(y_file)\n",
        "x = np.load(x_file)"
      ],
      "metadata": {
        "id": "luoI3IlrO7N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And make a scatter plot. Be sure to use a small marker size and make the dots semi-transparent!\n",
        "fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "ax.scatter(x, y, s=0.1, alpha=0.5)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')"
      ],
      "metadata": {
        "id": "HBZvxVMzYQYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine that you've taken this data from a physical variable you're very familiar with - and thus you know that you can model the variable with a 4th degree polynomial. Let's fit it quickly, e.g., with scikit-learn.\n",
        "\n",
        "#### **Q1) Model the data using a 4th degree polynomial**\n",
        "<details>\n",
        "<summary>Tips</summary>\n",
        "Scikit-learn doesn't include a polynomial regression model because they can be more easily made by extending the linear model! Since a polynomial regression is simply the linear combination of inputs to different powers, you can instead populate the feature space with these and fit a linear regression model.\n",
        "\n",
        "See <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\">this scikit-learn module</a> for more details.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "NmNuLRnNpSMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a 4th degree polynomial regression model using scikit-learn"
      ],
      "metadata": {
        "id": "NyP7XIVZ6eu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the polynomial regression and the datapoints"
      ],
      "metadata": {
        "id": "rM-4tgVK6z4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Here's one solution for fitting the model. Double click to see the answer.\n",
        "# %%script false --no-raise-error\n",
        "# Remove %%script false --no-raise-error in order to get this cell to run\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "poly_features = PolynomialFeatures(degree=4, include_bias=False)\n",
        "x_poly = poly_features.fit_transform(x.reshape(-1, 1))\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x_poly, y)\n",
        "\n",
        "x_pred = np.linspace(x.min(), x.max(), 500)\n",
        "x_pred_poly = poly_features.transform(x_pred.reshape(-1, 1))\n",
        "y_pred = lin_reg.predict(x_pred_poly)"
      ],
      "metadata": {
        "id": "WeybCf7Mw8lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Here's a quick plot of the polynomial regression in the solution above. Double click to see the answer.\n",
        "# %%script false --no-raise-error\n",
        "# Remove %%script false --no-raise-error in order to get this cell to run\n",
        "fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "ax.scatter(x, y, s=0.1, alpha=0.5)\n",
        "ax.plot(x_pred, y_pred, alpha=1, color='red')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')"
      ],
      "metadata": {
        "id": "TJTdciNKswxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does a pretty good job, but we can clearly see that y isn't a simple function of x - after all, there are multiple values of y for each value of x, so technically a function relating x to y does not exist!\n",
        "\n",
        "Thus, we can say that there is a degree of uncertainty in the value we predict for y. One common way to express this in your model is to add simple error bars, which quantify how far away your predicted y is from the actual value of y - **on average**.\n",
        "\n",
        "#### **Q2: How would express the confidence in this model?**\n",
        "Imagine a stakeholder asks you for a measurement of the error and how confident you are in the model predictions.\n",
        "\n",
        "<details>\n",
        "<summary>Tips</summary>\n",
        "\n",
        "Think of the many functions you already know for quantifying the error. Given the large number of samples that you have, how can you express the average error?\n",
        "\n",
        "Once you have a way of quantifying the error, how can you express your confidence in the spread of the error?\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "CVk6kvcBxPTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantify the error in the predictions made by the model"
      ],
      "metadata": {
        "id": "PRUmboF65b1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the model with your confidence intervals"
      ],
      "metadata": {
        "id": "IDwR0o0S5fxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary>Click here to read about one way you could answer question 1. Read it once you have your own answer ðŸ™‚ </summary>\n",
        "\n",
        "One way that you can quantify the error is by figuring out how far away your prediction is from the truth, on average. This can be done, e.g., by calculating the *mean absolute error*, i.e. $\\text{MAE} = \\dfrac{1}{n}\\sum_{i=0}^{n}|\\hat{y}_i - y_i|$.\n",
        "\n",
        "If you were to calculate the MAE and its statistics, as well as the variance of y, you'd get the following values (see our code cell below for the calculation)\n",
        "\n",
        "> The mean absolute error is 0.91 <br>\n",
        "The standard deviation of the error is 1.03 <br>\n",
        "The maximum error is 17.95 <br>\n",
        "The variance in y is 8.94\n",
        "\n",
        "If one were to see the report of this model including this MAE, one could get the misplaced impression that the model is able to capture the data's behavior quite well - an MAE of 0.91 compared to a variance of ~9 could give the appearance that the model does well enough (and for some applications, this could indeed be true).\n",
        "\n",
        "We can see, however, that the model does significantly worse at explaining the behavior of y around values of $x = 1$, $3$, and $5$. Similarly, by having a single value to represent the error mean and standard deviation, we are underconfident in our predictions in areas where our model does better (e.g.,  around x=2 & x=4).\n",
        "\n",
        "Like we discussed before, there can be many sources for the uncertainty we observe- for example, the spread in the values could be due to a stochastic process (i.e., a process we can only describe statistically), and this may lead us to have models that are better suited to predict conditions around specific inputs.\n",
        "</details>"
      ],
      "metadata": {
        "id": "2PhmH5-v1eNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Our solution for quantifying the average error.\n",
        "# Quantify the error using the mean absolute error.\n",
        "error = np.abs(lin_reg.predict(x_poly) - y)\n",
        "print(f'The mean absolute error is {error.mean():.2f}')\n",
        "print(f'The standard deviation of the error is {error.std():.2f}')\n",
        "print(f'The maximum error is {error.max():.2f}')\n",
        "print(f'The variance in y is {y.std():.2f}')"
      ],
      "metadata": {
        "id": "WUq9k_RS0vmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Our solution for visualizing the error.\n",
        "# Plot the polynomial model with error bars\n",
        "fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "ax.scatter(x, y, s=0.1, alpha=0.25)\n",
        "ax.plot(x_pred, y_pred, alpha=1, color='red', linewidth=0.5)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.fill_between(x_pred, y_pred - error.mean() - 2 * error.std(), y_pred + error.mean() + 2*error.std(), alpha=0.25, color='red')"
      ],
      "metadata": {
        "id": "bbesFHnWOIjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that simply using the average error for all $x$ is clearly unsatisfactory as we alternate between under-estimating and over-estimating the error ðŸ˜ž\n",
        "\n",
        "Looking a bit more closely ðŸ•µ it would seem as if we need a different error bar for each $x$.\n",
        "\n",
        "To get some intuition, let's look at the distribution of y values in our data near $x = 1.1$. Let's do this by finding all of the points that are within .02 of 1.1, and then plotting the histogram."
      ],
      "metadata": {
        "id": "piwe7zkfOI-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select x values within 0.01 of 1\n",
        "x_close = x[np.abs(x - 1.1) < 0.02]\n",
        "\n",
        "# Select the corresponding y values\n",
        "y_close = y[np.abs(x - 1.1) < 0.02]\n",
        "\n",
        "# Plot the distribution of y values\n",
        "fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "ax.hist(y_close, density=True)\n",
        "ax.set_xlabel('y')\n",
        "ax.set_ylabel('Probability density')\n",
        "ax.set_title('Distribution of y values for x â‰ˆ 1.1')"
      ],
      "metadata": {
        "id": "BV6RPtiZp-1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did as we did, you should get the following plot: <br><center>\n",
        "<!-- <img src=\"https://github.com/tbeucler/2024_MLEES_Ebook/blob/main/Milton/paste_link_here.png?raw=1\" width=80% height=250></center> -->\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "OHVyP-y0vH6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at this histogram, it looks like we could make the assumption that the distribution of y given a value of x can be approximated by a normal distribution (a.k.a. a bell curve, or a gaussian).\n",
        "\n",
        "The question now becomes, <i>how do we do this?</i> ðŸ¤”"
      ],
      "metadata": {
        "id": "SKiZJQYUHDES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distributional Regression"
      ],
      "metadata": {
        "id": "AI35gMJw0nIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run this code cell to define some functions we'll need for an interactive demo\n",
        "from ipywidgets import interact\n",
        "from ipywidgets import fixed\n",
        "def gaussian(mean, std):\n",
        "    \"\"\"\n",
        "    Plots a gaussian distribution with a given mean and standard deviation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : float\n",
        "        The mean of the gaussian distribution.\n",
        "    std : float\n",
        "        The standard deviation of the gaussian distribution.\n",
        "    \"\"\"\n",
        "    plt.close('all')\n",
        "    fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "    x = np.linspace(mean - 6*std, mean + 6*std, 500)\n",
        "    y = np.exp(-0.5 * (( x - mean) / std) ** 2) / (std * np.sqrt(2 * np.pi))\n",
        "    ax.plot(x, y)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'Gaussian with mean {mean} and std {std}')\n",
        "    ax.set_xlim(-30, 40)\n",
        "    ax.set_ylim(0, max(y.max()*1.1, 0.5))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Sx2glU7gczcj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to make this adjustment, we can write a model that predicts a distribution for each value of x. In this way, we are writing a model for the joint probability of x and y. As you may know, the shape of a normal distribution is given by two values: the mean (mu, $\\mu$) and the standard deviation (sigma, $\\sigma$).\n",
        "\n",
        "If you run the code below, you can play around with a gaussian distribution and see how the two parameters change the output."
      ],
      "metadata": {
        "id": "Pju6YnEJcycK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interact(gaussian, mean=(-5, 5, 0.1), std=(0.5, 10, 0.5))"
      ],
      "metadata": {
        "id": "T6dnDnYReHCb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that we can do this, why don't we train a neural network to predict the mean (*$\\mu$*) and standard deviation (*$\\sigma$*) of y given a value of x? Well, if you use the techniques we've been using so far, this might seem straightforward, until you reach the following question: <br> *what should I use as a training loss?*\n",
        "\n",
        "Before, you trained neural networks to reduce some kind of deterministic metric (e.g., the mean average error, root mean square error, or accuracy). However, these metrics require that we compare singular values against other singular values - if we predict $\\mu$ and $\\sigma$, we will need a function that will return a singular value when comparing distributions.\n",
        "\n",
        "Here is where the Continuous Ranked Probability Score (CRPS) comes in! Let's walk through what we're going to do in order for us to understand how our network will learn."
      ],
      "metadata": {
        "id": "21JJa_C90z27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explaining CRPS\n",
        "\n",
        "The CRPS is a value that quantifies the absolute area between the <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\"> cumulative distribution function</a> of two distributions. Let's say that we have two gaussians we want to compare to the standard gaussian distribution ($\\mu = 0$ and $\\sigma = 1$), the first with $\\mu = 0.5$ and $\\sigma = 1.2$, and the second with $\\mu=0.1$ and $\\sigma = 1.7$. Which of these two is closer to the standard gaussian?"
      ],
      "metadata": {
        "id": "36RM_AxwP6ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Let's plot the three gaussians. As always, feel free to check the code by double clicking :)\n",
        "from scipy.stats import norm\n",
        "fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "x = np.linspace(-5, 5, 500)\n",
        "y1 = norm(0, 1).pdf(x)\n",
        "y2 = norm(0.5, 1.2).pdf(x)\n",
        "y3 = norm(0.1, 1.7).pdf(x)\n",
        "\n",
        "colors = [np.array([215, 166, 122])/255, np.array([0, 148, 199])/255, np.array([214, 7, 114])/255]\n",
        "labels = ['Standard Gaussian', 'Gaussian 1', 'Gaussian 2']\n",
        "\n",
        "for i, y in enumerate([y1, y2, y3]):\n",
        "    ax.plot(x, y, color=colors[i], label=labels[i])\n",
        "    # shade the area underneath the curve\n",
        "    ax.fill_between(x, 0, y, color=colors[i], alpha=0.2)\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_title('Comparison of three gaussians')\n",
        "ax.legend()\n",
        "fig.tight_layout();"
      ],
      "metadata": {
        "id": "zEeSOSDTRIEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like we said, the CRPS depends on the area between the CDFs. Let's plot the CDFs of the standard Gaussian with the two Gaussians. Calculating the CRPS between two Gaussians requires a bit of math that we don't want to get into, so we'll just give you the function to calculate it ðŸ˜€"
      ],
      "metadata": {
        "id": "TAk2lgsmUj9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This cells defines the gaussian_crps function, which takes in $\\mu_1,\\; \\sigma_1,\\; \\mu_2,\\; \\sigma_2$\n",
        "from scipy.stats import norm\n",
        "def gaussians_crps(mu_F, sigma_F, mu_G, sigma_G):\n",
        "    \"\"\"\n",
        "    Calculates the CRPS between two gaussians.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu_F : float\n",
        "        The mean of the first gaussian.\n",
        "    sigma_F : float\n",
        "        The standard deviation of the first gaussian.\n",
        "    mu_G : float\n",
        "        The mean of the second gaussian.\n",
        "    sigma_G : float\n",
        "        The standard deviation of the second gaussian.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    crps : float\n",
        "        The CRPS between the two gaussians.\n",
        "    \"\"\"\n",
        "    # Calculate the intersection\n",
        "    x_inter = (mu_F * sigma_G - mu_G * sigma_F) / (sigma_G - sigma_F)\n",
        "\n",
        "\n",
        "    min_bound = min(mu_F - 4 * sigma_F, mu_G - 4 * sigma_G)\n",
        "    max_bound = max(mu_F + 4 * sigma_F, mu_G + 4 * sigma_G)\n",
        "\n",
        "    first_area = None\n",
        "    if x_inter > min_bound:\n",
        "        x_lower = np.linspace(min_bound, x_inter, 100)\n",
        "        # Calculate the CDF values for x_lower\n",
        "        y_lower_F = norm(mu_F, sigma_F).cdf(x_lower)\n",
        "        y_lower_G = norm(mu_G, sigma_G).cdf(x_lower)\n",
        "\n",
        "        # Calculate the area under the curve\n",
        "        first_area = np.abs(np.trapz(y_lower_F, x_lower) - np.trapz(y_lower_G, x_lower))\n",
        "\n",
        "    second_area = None\n",
        "    if x_inter < max_bound:\n",
        "        x_upper = np.linspace(x_inter, max_bound, 100)\n",
        "        # Calculate the CDF values for x_upper\n",
        "        y_upper_F = norm(mu_F, sigma_F).cdf(x_upper)\n",
        "        y_upper_G = norm(mu_G, sigma_G).cdf(x_upper)\n",
        "        # Calculate the area under the curve\n",
        "        second_area = np.abs(np.trapz(y_upper_F, x_upper) - np.trapz(y_upper_G, x_upper))\n",
        "\n",
        "    # Calculate the CRPS\n",
        "    crps = (first_area if first_area is not None else 0) + (second_area if second_area is not None else 0)\n",
        "\n",
        "    return crps"
      ],
      "metadata": {
        "id": "Ok4iHh04Y2YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run this cell to get an interactive plot visualizing the two distributions, the area between them, and the resulting CRPS.\n",
        "def compare_gaussians(mu_1=0.5, sigma_1=1.2, mu_2=0.1, sigma_2=1.7):\n",
        "    \"\"\"\n",
        "    Compares two gaussians.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu_1 : float\n",
        "        The mean of the first gaussian.\n",
        "    sigma_1 : float\n",
        "        The standard deviation of the first gaussian.\n",
        "    mu_2 : float\n",
        "        The mean of the second gaussian.\n",
        "    sigma_2 : float\n",
        "        The standard deviation of the second gaussian.\n",
        "    \"\"\"\n",
        "    plt.close('all')\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1, figsize = (6,5), dpi=100, sharex=True)\n",
        "\n",
        "    fig.set_facecolor(np.array([64,64,62])/255)\n",
        "    standard_gaussian = norm(0, 1)\n",
        "\n",
        "    x_lower = min(mu_1 - 4 * sigma_1, mu_2 - 4 * sigma_2)\n",
        "    x_upper = max(mu_1 + 4 * sigma_1, mu_2 + 4 * sigma_2)\n",
        "    x = np.linspace(x_lower, x_upper, 500)\n",
        "    ys = []\n",
        "    ys.append(standard_gaussian.cdf(x))\n",
        "    ys.append(norm(mu_1, sigma_1).cdf(x))\n",
        "    ys.append(norm(mu_2, sigma_2).cdf(x))\n",
        "\n",
        "    crps_scores = []\n",
        "    crps_scores.append(gaussians_crps(0, 1, mu_1, sigma_1))\n",
        "    crps_scores.append(gaussians_crps(0, 1, mu_2, sigma_2))\n",
        "\n",
        "    for idx, ax in enumerate(axs):\n",
        "        ax.plot(x, ys[0], color = colors[0], label=labels[0])\n",
        "        ax.plot(x, ys[idx+1], color=colors[idx+1], label=labels[idx+1], linewidth=0.1)\n",
        "        legend = ax.legend()\n",
        "\n",
        "\n",
        "\n",
        "        ax.fill_between(x, ys[0], ys[idx+1], color=colors[idx+1], alpha=0.2)\n",
        "        ax.set_title(f'Gaussian {idx+1} CRPS {crps_scores[idx]:.2f}')\n",
        "        ax.set_ylabel('y')\n",
        "        ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "        # Change the legend facecolor\n",
        "        legend.get_frame().set_facecolor(np.array([217, 217, 217])/255)\n",
        "        # And make the legend lines more visibile\n",
        "        for line in legend.get_lines():\n",
        "            line.set_linewidth(2)\n",
        "\n",
        "        # set the labels, ticks, tickmarks, axis labels, and title colors to white):\n",
        "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "                    ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "            item.set_color('white')\n",
        "        ax.tick_params(axis='both', which='both', color='white')\n",
        "\n",
        "        # draw axis lines\n",
        "        xaxis = ax.axhline(y=0, color='k', linestyle='--', linewidth=0.3)\n",
        "        yaxis = ax.axvline(x=0, color='k', linestyle='--', linewidth=0.3)\n",
        "\n",
        "        xaxis.set_dashes([20, 10])\n",
        "        yaxis.set_dashes([20, 10])\n",
        "\n",
        "    axs[1].set_xlabel('x')\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_gaussians.__doc__ = None\n",
        "\n",
        "interact(compare_gaussians,\n",
        "         mu_1=(-1, 1, 0.1), sigma_1=(1.1, 3, 0.1), # Parameter range of distribution 1\n",
        "         mu_2=(-1, 1, 0.1), sigma_2=(1.1, 3, 0.1), # Parameter range of distribution 2\n",
        "         )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ajg1V2b_UjaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We now have a way to evaluate the distance between two distributions (and it's even a [*proper score*](https://en.wikipedia.org/wiki/Scoring_rule#:~:text=Any%20given%20proper%20scoring%20rule,the%20predicted%20probabilities%20will%20ultimately) !!!) - but if you recall what we're going to be doing is comparing the gaussian that we predict to deterministic observations (i.e., we'll predict a distribution but our target is a single value).\n",
        "\n",
        "How do we go about doing this? Well, one way that we can do this is to represent our observations with what is known as a <a href='https://en.wikipedia.org/wiki/Degenerate_distribution'>*degenerate distribution*</a> (a.k.a. a dirac distribution), whose PDF is a step function with a value between 0 and 1, where the change happens at the value of our observation. Let's make a quick plot, since a picture is worth 1000 words!\n",
        "ðŸ“¸\n"
      ],
      "metadata": {
        "id": "SgwAvSMQsp7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run this cell to get an interactive plot of a degenerate distribution.\n",
        "def degenerate_distribution(x):\n",
        "    \"\"\"\n",
        "    Plots a degenerate distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : float\n",
        "        The value of the degenerate distribution.\n",
        "        \"\"\"\n",
        "    plt.close('all')\n",
        "    fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "\n",
        "    x_range = np.linspace(x - 5, x + 5, 500)\n",
        "    Y_heaviside = np.heaviside(x_range - x, 0)\n",
        "    ax.plot(x_range, Y_heaviside, color=np.array([215, 166, 122])/255)\n",
        "\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'Degenerate distribution with x = {x}')\n",
        "    ax.set_xlim(x-5, x+5)\n",
        "    ax.set_ylim(-.05, 1.05)\n",
        "    ax.axhline(y=0, color='k', linestyle='--', linewidth=0.3)\n",
        "    ax.axvline(x=0, color='k', linestyle='--', linewidth=0.3)\n",
        "    plt.show()\n",
        "\n",
        "interact(degenerate_distribution, x=(-4.9, 4.9, 0.1))"
      ],
      "metadata": {
        "id": "5F2EDFe4t3wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we have two distributions that we can compare using the CRPS! Let's take a single value of x in our dataset, and see if we can manually minimize the CRPS value for the observations for that single value of X."
      ],
      "metadata": {
        "id": "8oD-dkLW3tUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the files again. Note that they're npy files\n",
        "y = np.load(y_file)\n",
        "x = np.load(x_file)\n",
        "\n",
        "# 1.02 is one of the unique values in x, so let's use it\n",
        "# to get a sample of y_values.\n",
        "sample_x = 1.02\n",
        "\n",
        "# Let's get the corresponding y values\n",
        "sample_y = y[x == sample_x]\n",
        "\n",
        "# Print out some statistics\n",
        "mean = sample_y.mean()\n",
        "std = sample_y.std()\n",
        "print(f'There are {len(sample_y)} y values for x = {sample_x}')\n",
        "print(f'The mean of the sample is {mean:.2f} and the standard deviation is {std:.2f}')\n",
        "\n",
        "# and 5 points\n",
        "sample_y = np.random.choice(sample_y, 5)"
      ],
      "metadata": {
        "id": "KR9onydZ4Ox2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code based on the properscoring library\n",
        "# https://github.com/properscoring/properscoring/\n",
        "def crps_gaussian(y, mu, sig):\n",
        "    \"\"\"\n",
        "    Computes the CRPS of observations y relative to normally distributed\n",
        "    forecast with mean, mu, and standard deviation, sig.\n",
        "\n",
        "    CRPS(N(mu, sig^2); x)\n",
        "\n",
        "    Formula taken from Equation (5):\n",
        "\n",
        "    Calibrated Probablistic Forecasting Using Ensemble Model Output\n",
        "    Statistics and Minimum CRPS Estimation. Gneiting, Raftery,\n",
        "    Westveld, Goldman. Monthly Weather Review 2004\n",
        "\n",
        "    http://journals.ametsoc.org/doi/pdf/10.1175/MWR2904.1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : scalar or np.ndarray\n",
        "        The observation or set of observations.\n",
        "    mu : scalar or np.ndarray\n",
        "        The mean of the forecast normal distribution\n",
        "    sig : scalar or np.ndarray\n",
        "        The standard deviation of the forecast distribution\n",
        "    grad : boolean\n",
        "        If True the gradient of the CRPS w.r.t. mu and sig\n",
        "        is returned along with the CRPS.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    crps : scalar or np.ndarray or tuple of\n",
        "        The CRPS of each observation y relative to mu and sig.\n",
        "\n",
        "    \"\"\"\n",
        "    y = np.asarray(y)\n",
        "    mu = mu\n",
        "    sig = sig\n",
        "\n",
        "    # Normalize the y values using mu and sigma\n",
        "    y_norm = (y - mu) / sig\n",
        "\n",
        "    # Generate the standard gaussian\n",
        "    std_distribution = norm(0, 1)\n",
        "\n",
        "    # Calculate the pdf and cdf function values at which the target y value\n",
        "    # is located, as well as the constant value we'll need\n",
        "    pdf = std_distribution.pdf(y_norm)\n",
        "    cdf = std_distribution.cdf(y_norm)\n",
        "    c = 1. / np.sqrt(np.pi)\n",
        "\n",
        "    # Calculate the crps using the analytical formula from the reference\n",
        "    crps = sig * (y_norm * (2 * cdf - 1) + 2 * pdf - c)\n",
        "\n",
        "    return crps"
      ],
      "metadata": {
        "id": "Olixw49MPBz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def degenerate_gaussian(obs, mu = mean-1, sigma = std-.2):\n",
        "    \"\"\"\n",
        "    Plots a degenerate distribution cdf of a set of observations alongside a gaussian distribution cdf. Also outputs the CRPS value for each observation, as well as the mean CRPS value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    obs : np.ndarray\n",
        "        The observations.\n",
        "    mu : float\n",
        "        The mean of the gaussian distribution.\n",
        "    sigma : float\n",
        "        The standard deviation of the gaussian distribution.\n",
        "    \"\"\"\n",
        "    lower_bound = min(mu - 3 * sigma, np.min(obs).item() - 3 * sigma)\n",
        "    upper_bound = max(mu + 3 * sigma, np.max(obs).item() + 3 * sigma)\n",
        "\n",
        "    norm_distribution = norm(mu, sigma)\n",
        "    obs_range = np.linspace(lower_bound, upper_bound, 500)\n",
        "\n",
        "    cdf = norm_distribution.cdf(obs_range)\n",
        "\n",
        "    crps_vals = np.zeros(len(obs))\n",
        "    for idx, obs_val in enumerate(obs):\n",
        "        crps_vals[idx] = crps_gaussian(y=obs_val, mu=mu, sig=sigma)\n",
        "\n",
        "    num_rows = len(obs)\n",
        "    fig, axs = plt.subplots(num_rows, 1, figsize = (6,8), dpi=100, sharey=True)\n",
        "    fig.set_facecolor(np.array([64,64,62])/255)\n",
        "    for idx, ax in enumerate(axs):\n",
        "        degenerate_distribution = np.heaviside(obs_range - obs[idx], 0)\n",
        "        ax.plot(obs_range, cdf, color=colors[0])\n",
        "        ax.plot(obs_range, degenerate_distribution, color=np.array([0, 148, 199])/255)\n",
        "\n",
        "        ax.fill_between(obs_range, cdf, degenerate_distribution, color=colors[0], alpha=0.2)\n",
        "\n",
        "        # add label on the right side\n",
        "        # Add a label on the right border\n",
        "        label = f'CRPS: {crps_vals[idx]:.2f}'\n",
        "        ax.annotate(label, xy=(1, 0.5), xycoords='axes fraction',\n",
        "            xytext=(10, 0), textcoords='offset points',\n",
        "            ha='left', va='center', rotation=90, color='white')\n",
        "\n",
        "        # set the labels, ticks, tickmarks, axis labels, and title colors to white):\n",
        "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "                    ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "            item.set_color('white')\n",
        "        ax.tick_params(axis='both', which='both', color='white')\n",
        "\n",
        "    fig.suptitle(f'Distribution with mean {mu:.2f} and std {sigma:.2f}\\nMean CRPS: {crps_vals.mean():.2f}', color='white')\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Remove the docstring for a better user experience with interact()\n",
        "degenerate_gaussian.__doc__ = None\n"
      ],
      "metadata": {
        "id": "hni7X15BOboH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This cell will let you do what you will train your network to do! Can you find the mu and sigma that will minimize the value of the crps for these 5 observations?\n",
        "mu_min = sample_y.min()\n",
        "mu_max = sample_y.max()\n",
        "sigma_min = sample_y.std() / 6\n",
        "sigma_max = sample_y.std() * 6\n",
        "\n",
        "interact(degenerate_gaussian, mu=(mu_min, mu_max, 0.1), sigma=(sigma_min, sigma_max, 0.1), obs=fixed(sample_y))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L3_ay-P1NX0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Neural Network"
      ],
      "metadata": {
        "id": "Y2afKafqK9x1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we'll be modeling the conditional uncertainty by setting up a simple neural network using PyTorch."
      ],
      "metadata": {
        "id": "BOOi2IpRN_0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by importing torch, the functional api, and the module api\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "8pb5RPBDN-jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's create a model by defining a class that inherits from nn.module\n",
        "class ANN(nn.Module):\n",
        "    # Let's define what our model will look like using the init method\n",
        "    def __init__(self,\n",
        "                 input_size, # the number of features in the input\n",
        "                 hidden_size, # the number of neurons in the hidden layer\n",
        "                 output_size # the number of output features.\n",
        "                 ):\n",
        "        super(ANN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # When the model is trained or used in inference, it calls upon its 'forward' method, which tell's it precisely what it needs to do. Let's go ahead and define it\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Lwy906cqPVl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown CRPS isn't predefined as a loss function in torch, but it *can* be written using torch. We've gone ahead and set it up as ```CRPS_ML``` for you in this cell,\n",
        "\n",
        "def CRPS_ML(y_pred, y_true, **kwargs):\n",
        "    \"\"\"Compute the Continuous Ranked Probability Score (CRPS).\n",
        "\n",
        "    The CRPS is a probabilistic metric that evaluates the accuracy of a\n",
        "    probabilistic forecast. It is defined as the integral of the squared\n",
        "    difference between the cumulative distribution function (CDF) of the\n",
        "    forecast and the CDF of the observations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    y_pred : array-like of shape (n_samples, 2*n_features)\n",
        "        The predicted probability parameters for each sample. The odd columns\n",
        "        should contain the mean (mu), and the even columns should contain the\n",
        "        standard deviation (sigma).\n",
        "\n",
        "    y_true : array-like of shape (n_samples,n_features)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    crps : float or array-like\n",
        "        The CRPS value mean, or array of individual CRPS values.\n",
        "\n",
        "    \"\"\"\n",
        "    # adapted from https://github.com/WillyChap/ARML_Probabilistic/blob/main/Coastal_Points/Testing_and_Utility_Notebooks/CRPS_Verify.ipynb\n",
        "    # and work by Louis Poulain--Auzeau (https://github.com/louisPoulain)\n",
        "    reduction = kwargs.get(\"reduction\", \"mean\")\n",
        "    mu = y_pred[:, ::2]\n",
        "    sigma = y_pred[:, 1::2]\n",
        "\n",
        "    # prevent negative sigmas\n",
        "    sigma = torch.sqrt(sigma.pow(2))\n",
        "    loc = (y_true - mu) / sigma\n",
        "    pdf = torch.exp(-0.5 * loc.pow(2)) / torch.sqrt(\n",
        "        2 * torch.from_numpy(np.array(np.pi))\n",
        "    )\n",
        "    cdf = 0.5 * (1.0 + torch.erf(loc / torch.sqrt(torch.tensor(2.0))))\n",
        "\n",
        "    # compute CRPS for each (input, truth) pair\n",
        "    crps = sigma * (\n",
        "        loc * (2.0 * cdf - 1.0)\n",
        "        + 2.0 * pdf\n",
        "        - 1.0 / torch.from_numpy(np.array(np.sqrt(np.pi)))\n",
        "    )\n",
        "\n",
        "    return crps.mean() if reduction == \"mean\" else crps"
      ],
      "metadata": {
        "id": "FJQX-tjhRK1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define the other aspcets for our model training in torch as well. This includes the batch size, optimizer, & loss function. Let's make a list of the things we need to do:\n",
        "\n",
        "1. Since we know the underlying relationship is well represented using 4th degree polynomials, let's make polynomial features of the x array\n",
        "2. Convert the numpy arrays to pytorch tensors.\n",
        "3. Convert the tensors into a tensor dataset and use the dataset to make a dataloader.\n",
        "4. Instantiate the model. Remember that the\n",
        "5. Define the loss function and optimizer\n",
        "6. Write and run the training loop. It should include an early stopping criterion or two :\n"
      ],
      "metadata": {
        "id": "rfS2-LbHX4us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_fitter = PolynomialFeatures(degree=4, include_bias=False)\n",
        "poly_x = poly_fitter.fit_transform(x.reshape(-1, 1))\n",
        "\n",
        "x_tensor = torch.from_numpy(poly_x).float()\n",
        "y_tensor = torch.from_numpy(y[:,None]).float()\n",
        "print(x_tensor.shape, y_tensor.shape)\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
        "dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                         batch_size=256, # The number of samples in each batch\n",
        "                                         shuffle=True, # Whether to shuffle the order of the points being fed during training\n",
        "                                         )"
      ],
      "metadata": {
        "id": "iO6y-EdRQz1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ANN(4, #Number of input features\n",
        "            110, # Number of neurons / hidden layer\n",
        "            2, # Predicted features, =2 because we predict a mean and std\n",
        "            )"
      ],
      "metadata": {
        "id": "QKV6lm1qZsPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = CRPS_ML\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "LkKjDeEWtFzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "print('Starting training')\n",
        "for epoch in range(num_epochs):\n",
        "    batch_losses = []\n",
        "    i = 0\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        i+=1\n",
        "        print(f'\\rEpoch {epoch+1}/{num_epochs}, Batch {i}/{len(dataloader)}, last loss: {losses[-1] if len(losses)>=1 else 0}', end='', flush=True)\n",
        "\n",
        "        # Reset the gradients - needs to happen for each iteration\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        y_pred = model(x_batch)\n",
        "        # Compute the loss\n",
        "        loss = loss_function(y_pred, y_batch)\n",
        "        batch_losses.append(loss.item())\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "    losses.append(np.mean(batch_losses))\n",
        "\n",
        "    # save the best model\n",
        "    if len(losses) > 1:\n",
        "        if losses[-1] < min(losses[:-1]):\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    # early stopping\n",
        "    if len(losses) >=10:\n",
        "        if np.mean(losses[-10:]) < losses[-1]:\n",
        "            break\n",
        "\n",
        "print('\\n Training complete')"
      ],
      "metadata": {
        "id": "AareuAqvtUah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training curve\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B_z8yQyz4Srl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance Overview"
      ],
      "metadata": {
        "id": "ebVzq28gImdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's go ahead and load the best model state from what we saved before\n",
        "model.load_state_dict(torch.load('best_model.pth'))"
      ],
      "metadata": {
        "id": "ZnYylBN66eQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's go ahead and generate the plot for our predicted distributions\n",
        "x_plot = np.linspace(x.min(), x.max(), 500)\n",
        "x_plot = poly_fitter.fit_transform(x_plot[:,None])\n",
        "\n",
        "x_tensor_plot = torch.from_numpy(x_plot).float()\n",
        "y_pred = model(x_tensor_plot)\n",
        "\n",
        "mu = y_pred[:, 0].detach().numpy()\n",
        "sigma = np.abs(y_pred[:, 1].detach().numpy())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, label='Data', alpha=0.15, s=0.05, color = colors[1])\n",
        "\n",
        "#plot the mean\n",
        "plt.plot(x_plot[:,0], mu, label='Mean', color=colors[2], linewidth=0.7)\n",
        "\n",
        "# plot 2 standard deviations above and below\n",
        "plt.fill_between(x_plot[:,0], mu + 2*sigma, mu - 2*sigma, color=colors[2], alpha=0.2)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Predicted Distribution')\n",
        "# Increase the weight of the lines in the legend\n",
        "legend = plt.legend()\n",
        "for line in legend.get_lines():\n",
        "    line.set_linewidth(2)\n",
        "# increase the marker size of the scatter elements in the legend\n",
        "legend.legend_handles[0]._sizes = [60];\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f7STcgOv80JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigma.min()"
      ],
      "metadata": {
        "id": "ggC1YWjlzEaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Now that we have a probabilistic model, we need to discuss ways in which to evaluate how well our model performs. However, up to now we've focused on deterministic metrics (e.g., RMSE, accuracy, MAE) in order to evaluate the performance of our models. As we saw before, these metrics are less useful when we predict a whole distribution as our outputs (whether it be via distributional regression as we did above, or even when making predictions with ensembles).\n",
        "\n",
        "Furthermore, we used the one probabilistic metric we've discussed as our optimization target (i.e., we trained the model to minimize the CRPS) -  if we use it as an evaluation metric we'll get an overly optimistic view of how well our model performs. We're therefore going to introduce two more metrics to evaluate the performance of our model.\n",
        "\n",
        "Note that these metrics also work when using other methods for quantifying uncertainty, such as cross-validation or model ensembles. However, the scores will be much worse for models whose errors aren't conditional on the inputs!"
      ],
      "metadata": {
        "id": "PDWdGQsmN947"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spread Skill Score\n",
        "The Spread Skill Score (SSC) is used to quantify how large the *spread* predicted by your model(s) is (e.g., the predicted sigma in our case) compared to the mean error associated with predicted mean (i.e., its *skill*)."
      ],
      "metadata": {
        "id": "gOKqGKu0URnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's start by calculating the unique values of X. Note that normally this would done, e.g., by carrying out a random selection of inputs in order to sample the predicted spread and error. However, here we have the luxury of knowing that we have a relatively range of unique values for our univariate input.\n",
        "unique_x = np.unique(x)\n",
        "\n",
        "spread = np.zeros_like(unique_x)\n",
        "error = np.zeros_like(unique_x)\n",
        "# For each unique X value, let's get the predicted spread and the MAE\n",
        "for idx, x_val in enumerate(unique_x):\n",
        "    # Get the corresponding y values\n",
        "    y_true = y[x==x_val]\n",
        "\n",
        "    # Generate the mean and std from the model\n",
        "    mu, sigma = model(torch.from_numpy(poly_fitter.fit_transform(np.asarray(x_val)[None,None])).float()).detach().numpy().squeeze()\n",
        "\n",
        "    # Make sure that sigma is always positive\n",
        "    sigma = np.sqrt(sigma**2)\n",
        "\n",
        "    # Calculate the spread and error\n",
        "    spread[idx] = sigma\n",
        "    error[idx] = np.abs((y_true - mu)).mean() # MAE\n",
        "\n",
        "# Sort the spread and error by increasing error\n",
        "idxs = np.argsort(error)\n",
        "error = error[idxs]\n",
        "spread = spread[idxs]"
      ],
      "metadata": {
        "id": "lc6b6w4yUWjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a square figure using plt.subplots\n",
        "fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
        "\n",
        "# set the origin to 0,0 and the top right corner to max(error_max, spread_max)\n",
        "upper_bound = max(error.max(), spread.max()) * 1.1\n",
        "ax.set_xlim(0, upper_bound)\n",
        "ax.set_ylim(0, upper_bound)\n",
        "\n",
        "# draw the diagonal line\n",
        "ax.plot([0, upper_bound],\n",
        "        [0, upper_bound],\n",
        "        color='black',\n",
        "        linestyle='--')\n",
        "\n",
        "# plot the spread vs skill\n",
        "ax.scatter(error, spread, s=0.75, color=colors[2])"
      ],
      "metadata": {
        "id": "iLqp1MIAjunc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the ideal spread skill score is a line where the error in your prediction is equal to the spread that you predict!\n",
        "\n",
        "Additionally, if you use a method of uncertainty quantification that isn't conditional on the inputs (like the error bars we produced way up in Question 2), you'll have a vertical line whose intersection with the diagonal will be whereever your error is as large as the spread in the data!"
      ],
      "metadata": {
        "id": "wqSk-L_cIjQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Probability Integral Transform (PIT) Histogram\n",
        "While this visualization carries a name that some may find intimidating (one of your T.A.'s will readily admit to waking from a nightmare muttering something about it with a far off look on his face), it's simply a way of quantifying how well calibrated your models are. It's simply a matter of calculating the CDF of random samples from the observations and plotting a histogram."
      ],
      "metadata": {
        "id": "HWQ_Uxl4UWzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# First, we need to generate a set of y values to get a global distribution of y-values.\n",
        "x_vals = np.linspace(x.min(), x.max(), 500)\n",
        "\n",
        "y_samples = []\n",
        "for x_val in x_vals:\n",
        "    # Let's generate mu and sigma\n",
        "    mu, sigma = model(torch.from_numpy(poly_fitter.fit_transform(np.asarray(x_val)[None,None])).float()).detach().numpy().squeeze()\n",
        "    # Make sure that sigma is always positive\n",
        "    sigma = np.sqrt(sigma**2)\n",
        "\n",
        "    # make the distribution using scipy and get 100 samples\n",
        "    y_samples.append(norm(mu, sigma).rvs(100))\n",
        "\n",
        "y_samples = np.concatenate(y_samples)"
      ],
      "metadata": {
        "id": "NMk0hRRw25Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the predicted values for y, let's go ahead and get an estimation of the CDF of y. To do this, we'll generate a regularly spaced grid of y values going from the minimum (where CDF~0) to the maximum (where CDF~1).\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "y_grid = np.linspace(y.min(), y.max(), 500)\n",
        "cdf_grid = np.zeros_like(y_grid)\n",
        "for idx, y_val in enumerate(y_grid):\n",
        "    cdf_grid[idx] = np.mean(y_samples <= y_val)\n",
        "\n",
        "# and then we make an interpolator\n",
        "cdf_interpolator = interp1d(y_grid,\n",
        "                            cdf_grid,\n",
        "                            kind='linear',\n",
        "                            )"
      ],
      "metadata": {
        "id": "ZUJnQtD13zr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to make the PIT histogram, all we have to do is get a large number of samples, and plot the PDF of the CDF values calculated from the y_true samples.\n",
        "# If we obtain a relatively uniform PDF, it means that our predictions do a good job of matching the CDF of the observations.\n",
        "\n",
        "rn_gen = np.random.default_rng(seed=42)\n",
        "\n",
        "samples = rn_gen.choice(y, size=8000)\n",
        "CDF_vals = cdf_interpolator(samples)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
        "\n",
        "nbins = 10\n",
        "# Plot the histogram of the CDF values using 10 bins\n",
        "ax.hist(CDF_vals, bins=nbins, density=True, color=colors[1], label='PIT Histogram')\n",
        "# set the x-axis ticks to be every 0.05 between 0 and 1\n",
        "ax.set_xticks(np.arange(0.05, 1.1, 0.1))\n",
        "# plot the hline at 1\n",
        "ax.axhline(1, color='black', linestyle='--', label='Uniform PDF')\n",
        "ax.set_xlim(0, CDF_vals.max())\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "DkaK6DP49CRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Is the model over-dispersive or under-dispersive?"
      ],
      "metadata": {
        "id": "E3mIlQVsLtnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus Challenge"
      ],
      "metadata": {
        "id": "W8vQKBivsYQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can imagine, the data that you used today does *not*, in fact, associate a gaussian distribution of values for each value of x. Instead, the distribution is a *weibull* distribution whose shape and scale parameters depend on x!\n",
        "\n",
        "We've gone ahead and prepared an interactive demo for you to get familiar with the weibull distribution."
      ],
      "metadata": {
        "id": "M5b0oMMiGd0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weibull(shape, scale):\n",
        "    plt.close('all')\n",
        "    fig, ax = plt.subplots(figsize = (8,2), dpi=150)\n",
        "    x = np.linspace(1e-5, 100, 500)\n",
        "    y = (shape / scale) * (x / scale) ** (shape - 1) * np.exp(-(x / scale) ** shape)\n",
        "    ax.plot(x, y)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(f'Weibull with shape {shape} and scale {scale}')\n",
        "    ax.set_xlim(0, 40)\n",
        "    ax.set_ylim(0, max(y.max()*1.1, 0.5))\n",
        "    plt.show()\n",
        "\n",
        "interact(weibull, shape=(1e-5, 10, 0.1), scale=(0.5, 10, 0.5))"
      ],
      "metadata": {
        "id": "AUM3uL-KeS6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The challenge, however, is to repeat the fitting of the neural network by predicting a weibull instead of a gaussian distribution. Note that this means that **the CRPS we implemented above is not correct for use with the weibull distribution** - and as your TA ran out of time and energy to figure it out for you, it's up to you to figure it out.\n",
        "\n",
        "\"You really shouldn't spend too much time on this. I'd rather you work on your final project, or have a coffee\" - that same TA."
      ],
      "metadata": {
        "id": "xRHrGaLmHeq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may ignore your TA and implement a CRPS loss tailored to the Weibull distribution below"
      ],
      "metadata": {
        "id": "I2OwTZvCJwX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your model with the new CRPS loss"
      ],
      "metadata": {
        "id": "MYOgmZnXJ-Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot its predictions as a function of x"
      ],
      "metadata": {
        "id": "q1aQSw3HKAS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does its spread skill score look like? Does the spread match the error?"
      ],
      "metadata": {
        "id": "udkFf2tqLnBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the PIT look like? Is the model over-dispersive, under-dispersive, or just right?"
      ],
      "metadata": {
        "id": "DucwkDP9MAbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}