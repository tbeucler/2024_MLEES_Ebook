{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2024_MLEES_Ebook/blob/main/Kejdi/10_Hybird_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOhJgImVk7fh"
      },
      "source": [
        "## (Exercise) Introduction to Hybrid models. Combining Physics-Based and Machine Learning Models\n",
        "\n",
        "\n",
        "### Learning Objectives:\n",
        "1. **How can we model glaciers** with physic-based approach?\n",
        "2. **Understand what hybrid models are** and how they integrate physics-based and ML approaches.\n",
        "3. **Explore the advantages and shortcomings** of using hybrid models.\n",
        "4. **Examine different applications** for combining machine learning and physical models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzDc9Wi2k7fi"
      },
      "source": [
        "**Q: According to you, what is a glacier ?**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://home.nps.gov/lacl/learn/nature/images/Glaciers-page_-Image-w-cred-cap_-1200w_-Tanaina-Glacier_2_1.jpg?maxwidth=1300&autorotate=false&quality=78&format=webp\" width=80%></img>\n",
        "\n",
        "<br> <i> To me :  A glacier is a non-Newtonian fluid, with a stress dependent viscosity, that flows from higher to lower elevations </i>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPwHr8yik7fj"
      },
      "source": [
        "#### Introduction to Glacier Flow\n",
        "Glaciers are massive bodies of ice that flow slowly under their own weight. The flow of ice in a glacier is a complex process governed by physical laws that describe how the ice deforms and moves.\n",
        "Given an initial glacier geometry, the time evolution in ice thickness $h(x, y, t)$ is determined by the mass conservation equation, which couples ice dynamics and surface mass balance (SMB)\n",
        "through:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "    \\frac{\\partial h}{\\partial t} + \\nabla \\cdot (\\mathbf{u}h) = SMB,\n",
        "     \\\\\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "where $\\nabla \\cdot$ denotes the divergence operator with respect to the flux ($Q=\\mathbf{u}h$). $\\mathbf{u}$ is the vertically averaged horizontal ice velocity field and SMB the SMB function, which consists of the\n",
        "integration of ice accumulation and ablation over one year.\n",
        "\n",
        "Mass conservation equation is generic and can be applied to model glacier evolution in number of applications provided adequate SMB and ice-flow model components.\n",
        "In the following, we mostly focus on developing an efficient numerical method to compute the ice-flow considering it is often the most computationally expensive component in glacier evolution model.\n",
        "\n",
        "- First, we will use a numerical solution to compute $Q$.\n",
        "- Then we will use a data-driven Machine Learning (ML) approach that emulates ice-flow.\n",
        "The state of the art ML techniques, use a Physics Informed Neural Network (PINN); however, this goes beyond the scope of this chapter which focuses on hybrid models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH5XMcpNk7fj"
      },
      "source": [
        "#### Numerical solution for **u**\n",
        "\n",
        "We can describe the ice-flow equations under stress, but their computational cost makes them impractical for large-scale ice-sheet modeling over long time periods. This leads us to introduce the **shallow ice approximation (SIA)**, which is used in ice sheet models due to its simplicity and efficiency.\n",
        "\n",
        "The SIA simplifies the full Stokes equations by assuming that the ice flow is dominated by vertical shear stresses and that horizontal stresses can be neglected. This results in an expression for the ice velocity in terms of the ice thickness gradient and the surface slope. The ice velocity $\\mathbf{u}$ can be approximated as:\n",
        "\n",
        "$\n",
        "\\mathbf{u} = -\\left(\\frac{2 A}{n+2}\\right) \\left(\\rho g \\sin(s)\\right)^n h^{n+1} \\nabla h,\n",
        "$\n",
        "\n",
        "where:\n",
        "- $\\rho$ is the ice density,\n",
        "- $g$ is the gravitational acceleration,\n",
        "- $A$ is Glen's flow rate factor,\n",
        "- $n$ is Glen's law exponent (typically 3),\n",
        "- $h$ is the ice thickness, and\n",
        "- $s$ is the surface slope.\n",
        "\n",
        "This equation governs the horizontal velocity of ice based on the local ice thickness and slope.\n",
        "We plug the **u** into the mass conservation equation and obtain:\n",
        "\n",
        "$\n",
        "\\frac{\\partial h}{\\partial t} + \\nabla \\cdot (D(h,z)\\frac{\\partial z}{\\partial x}) = \\text{SMB},\n",
        "$\n",
        "\n",
        "where $D(h,z) =  f_d (\\rho g)^3 h^5 |\\nabla S|^2$.\n",
        "\n",
        "This equation describes the time evolution of the ice thickness, where the velocity $\\mathbf{u}$ is computed from the ice sheet's surface slope and thickness. This approach provides a balance between accuracy and computational efficiency and is widely used in large-scale ice sheet models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20U9vjYVk7fj"
      },
      "source": [
        "##### Boundary Conditions with no-slip condition at the case\n",
        "In many glacier models, we assume a no-slip condition at the base, meaning the ice velocity is zero at the bedrock. This condition is suitable for glaciers frozen to their beds:\n",
        "\n",
        "$\n",
        "\\mathbf{u} = 0 \\text{ on the bedrock surface}.\n",
        "$\n",
        "\n",
        "**Stress-Free Surface**\n",
        "\n",
        "At the glacier surface (exposed to air), a stress-free boundary condition is typically applied:\n",
        "\n",
        "$\n",
        "\\sigma \\cdot \\mathbf{n} = 0 \\text{ on the surface},\n",
        "$\n",
        "\n",
        "Where $\\mathbf{n}$ is the outward normal vector at the glacier surface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LxlqM02k7fj",
        "outputId": "4826307d-f6d1-41e3-d41d-4931040df820"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-13 10:08:11.366634: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-13 10:08:11.411015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:11.411026: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "#import the necessary libraies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "import netCDF4\n",
        "from IPython.display import clear_output\n",
        "import pooch\n",
        "\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Make True if you want to train the model from scratch. It should take 30 min to train.\n",
        "train_mode=True\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuvseTy8k7fl"
      },
      "outputs": [],
      "source": [
        "# Physical parameters\n",
        "Lx = 49700    # Domain length in x (m)\n",
        "Ly = 32300    # Domain length in y (m)\n",
        "ttot = 700   # Time limit (yr)\n",
        "grad_b = 0.001 # Mass balance gradient (no unit)\n",
        "b_max = 0.5   # Maximum precip (m/yr)\n",
        "Z_ELA = 3000  # Elevation of equilibrium line altitude (m)\n",
        "rho = 910.0   # Ice density (g/m^3)\n",
        "g   = 9.81    # Earth's gravity (m/s^2)\n",
        "fd  = 1e-18   # Deformation constant (Pa^-3 y^-1)\n",
        "\n",
        "\n",
        "\n",
        "# Initialization & load data\n",
        "\n",
        "nout = 50  # Frequency of plotting\n",
        "dtmax = 1   # maximum time step\n",
        "dt = dtmax  # Initial time step\n",
        "dx = 100\n",
        "dy = 100  # Cell size in y\n",
        "nx=int(Lx/dx)\n",
        "ny=int(Ly/dy)\n",
        "x = np.linspace(0, Lx, nx)  # x-coordinates\n",
        "y = np.linspace(0, Ly, ny)  # y-coordinates\n",
        "\n",
        "\n",
        "bedrock_url='https://unils-my.sharepoint.com/:u:/g/personal/kejdi_lleshi_unil_ch/EXI_z9iu_MlMn_J4IdG97DkBpvE8K-IKiZUxuEogU-cwVg?download=1'\n",
        "hash = None\n",
        "file = pooch.retrieve(bedrock_url, known_hash=hash)\n",
        "nc_file = netCDF4.Dataset(file)  # Load the NetCDF file\n",
        "Z_topo = nc_file.variables['topg']    # Replace 'topg' with the appropriate v\n",
        "\n",
        "H_ice = np.zeros((ny, nx))  # Initial ice thickness\n",
        "Z_surf = Z_topo + H_ice  # Initial ice surface\n",
        "time = 0  # Initial time\n",
        "it = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuHFSC-8k7fl",
        "outputId": "0f0a2182-a433-4349-a7a5-12fbf141314c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 2200x800 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Loop\n",
        "while time < ttot:\n",
        "\n",
        "    # Update time\n",
        "    time += dt\n",
        "    it   += 1\n",
        "\n",
        "    # Calculate H_avg, size (ny-1,nx-1)\n",
        "    H_avg = 0.25 * (H_ice[:-1, :-1] + H_ice[1:, 1:] + H_ice[:-1, 1:] + H_ice[1:, :-1])\n",
        "\n",
        "    # Compute Snorm, size (ny-1,nx-1)\n",
        "    Sx = np.diff(Z_surf, axis=1) / dx\n",
        "    Sy = np.diff(Z_surf, axis=0) / dy\n",
        "    Sx = 0.5 * (Sx[:-1, :] + Sx[1:, :])\n",
        "    Sy = 0.5 * (Sy[:, :-1] + Sy[:, 1:])\n",
        "    Snorm = np.sqrt(Sx**2 + Sy**2)\n",
        "\n",
        "    # Compute D, size (ny-1,nx-1)\n",
        "    D = fd * (rho * g)**3.0 * H_avg**5 * Snorm**2\n",
        "\n",
        "    # Compute dt\n",
        "    dt = min(min(dx, dy)**2 / (4.1 * np.max(D)), dtmax)\n",
        "\n",
        "    # Compute qx, size (ny-2,nx-1)\n",
        "    qx = -(0.5 * (D[:-1,:] + D[1:,:])) * np.diff(Z_surf[1:-1,:], axis=1) / dx\n",
        "\n",
        "    # Compute qy, size (ny-1,nx-2)\n",
        "    qy = -(0.5 * (D[:,:-1] + D[:,1:])) * np.diff(Z_surf[:,1:-1,], axis=0) / dy\n",
        "\n",
        "    # Update rule (diffusion)\n",
        "    dHdt = -(np.diff(qx, axis=1) / dx + np.diff(qy, axis=0) / dy)\n",
        "    H_ice[1:-1, 1:-1] += dt * dHdt # size (ny-2,nx-2)\n",
        "\n",
        "    b = np.minimum(grad_b * (Z_surf - Z_ELA), b_max)\n",
        "\n",
        "    # Update rule (mass balance)\n",
        "    H_ice[1:-1, 1:-1] += dt * b[1:-1, 1:-1]\n",
        "\n",
        "    # Update rule (positive thickness)\n",
        "    H_ice = np.maximum(H_ice, 0)\n",
        "\n",
        "    # updatesurface topography\n",
        "    Z_surf = Z_topo + H_ice\n",
        "\n",
        "    # Update ELA after 500 years\n",
        "    if time > 500:\n",
        "        Z_ELA = 2700\n",
        "\n",
        "    # Display\n",
        "    if it % nout == 0:\n",
        "        clear_output(wait=True)  # Clear the previous output in the notebook\n",
        "\n",
        "        plt.figure(2, figsize=(11, 4), dpi=200)\n",
        "\n",
        "        # First subplot: Ice surface\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(Z_surf, extent=[0, Lx/1000, 0, Ly/1000], cmap='terrain', origin='lower')\n",
        "        plt.colorbar(label='Elevation (m)')\n",
        "        plt.title('Ice Surface at ' + str(int(time)) + ' y')\n",
        "        plt.xlabel('Distance, km')\n",
        "        plt.ylabel('Distance, km')\n",
        "\n",
        "        # Second subplot: Ice thickness\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(np.where(H_ice > 0, H_ice, np.nan), extent=[0, Lx/1000, 0, Ly/1000], cmap='jet', origin='lower')\n",
        "        plt.colorbar(label='Ice Thickness (m)')\n",
        "        plt.title('Ice Thickness at ' + str(int(time)) + ' y')\n",
        "        plt.xlabel('Distance, km')\n",
        "        plt.ylabel('Distance, km')\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSgxi9vIk7fl"
      },
      "source": [
        "**Q: Which is the most expensive part of of solving the mass conservation equation?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzotimcek7fl"
      },
      "source": [
        "<i>The most expensive part is the calculation of **u**. </i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX2eJcNkk7fl"
      },
      "source": [
        "### Introduction to hybrid modeling\n",
        "Hybrid models combine traditional physics-based models (model-based, MB) with machine learning (ML) to leverage the strengths of both approaches. In purely physics-based models, known equations govern system dynamics, but these models often require detailed domain knowledge and can be limited by the availability of precise parameters. Machine learning, in contrast, can model complex systems without relying on such parameters, making it useful for data-rich but theory-poor domains. However, ML models may struggle to generalize outside the data they are trained on and can produce results that violate known physical laws."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUfTvBAak7fm"
      },
      "source": [
        "\n",
        "**Q: How can hybrid models overcome these limitations?**\n",
        "\n",
        "<i>Hybrid modeling overcomes these limitations by using physics-informed constraints, embedding known physical equations into machine learning models, or combining the outputs of both approaches. The goal is to create models that are more accurate and robust, especially in cases with limited data or imperfect physical models. By fusing physics with data-driven methods, hybrid models can handle sparse data, correct ML predictions that violate physical laws, and produce interpretable results across a wide range of applications.</i>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukTOAaYrk7fm"
      },
      "source": [
        "**Q: How can we make our simulations run faster?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmfcV54uk7fm"
      },
      "source": [
        "<i>We can replace the calutation of **u** with an emulator. The emulator will take as input the state of the medium (glacier thickness, slope of glacier, etc) and will calucate the velocity field (**u**) for the corresponding time step. </i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRA_BjV4k7fm"
      },
      "source": [
        "\n",
        "#### Emulating Ice Flow with Machine Learning\n",
        "\n",
        "The *Instructed Glacier Model* ([IGM](https://github.com/jouvetg/igm)) introduces a convolutional neural network (CNN) to predict ice flow, trained using data from traditional models such as hybrid SIA+SSA or Stokes models. The advantage of this approach is that it substitutes the computationally expensive ice flow component with a much faster emulator. This enables simulations that are up to 1000 times faster, with a fidelity of over 90%.\n",
        "\n",
        "#####  Overview of the Machine Learning Approach\n",
        "\n",
        "A simple version of the IGM could be built by the following steps:\n",
        "1. **Input Variables**: The ML model takes ice thickness and surface slope gradients as inputs  $\\{ h(x,y), \\frac{\\partial s}{\\partial x},\\frac{\\partial s}{\\partial y}\\}$.\n",
        "2. **Training**: A convolutional neural network (CNN) is trained using a dataset generated from high-order glacier flow models.\n",
        "3. **Prediction**: Once trained, the emulator predicts the vertically averaged ice flow from the input variables **u**(u,v).\n",
        "    \n",
        " The input and output fields are 2 D grid rasters:\n",
        "\n",
        "$\n",
        "\\R^{N_x \\times N_y \\times 3} \\rightarrow \\R^{N_x \\times N_y \\times 2} ,\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26qbsHfdk7fm"
      },
      "source": [
        "**Q: what is another advantage of the hybrid model (except making faster models)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiF-Mv-Ak7fm"
      },
      "source": [
        "<i>Hybrid models can help in theory-poor domains where we do not have known equation that govern system dynamics. However, we should have enough data and be sure to make this model generalizable. A pitfall, when we dont have enough data and are not carefull, could be to overfitting</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KuZGT47k7fm"
      },
      "source": [
        "##### Necessary funcitons you do not need to know"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qys2XZW9k7fm"
      },
      "outputs": [],
      "source": [
        "#@markdown Test\n",
        "\n",
        "# Function to load NetCDF files\n",
        "\n",
        "def load_from_pooch(file):\n",
        "    nc_file = xr.open_dataset(file)\n",
        "    return nc_file\n",
        "\n",
        "\n",
        "# Function to scale the field based on 90th percentile of its maximum\n",
        "def scale_field(field):\n",
        "    max_90th_percentile = np.percentile(field.max(axis=(1, 2)), 90)\n",
        "    return field / max_90th_percentile\n",
        "\n",
        "# Function to augment the data by flipping and adding noise\n",
        "def augment_data(inputs, outputs):\n",
        "    aug_inputs, aug_outputs = [], []\n",
        "\n",
        "    for inp, out in zip(inputs, outputs):\n",
        "        # Original data\n",
        "        aug_inputs.append(inp)\n",
        "        aug_outputs.append(out)\n",
        "\n",
        "        # Horizontal flip\n",
        "        aug_inputs.append(np.flip(inp, axis=2))  # Flip along x-axis\n",
        "        aug_outputs.append(np.flip(out, axis=2))\n",
        "\n",
        "        # Vertical flip\n",
        "        aug_inputs.append(np.flip(inp, axis=1))  # Flip along y-axis\n",
        "        aug_outputs.append(np.flip(out, axis=1))\n",
        "\n",
        "    return np.array(aug_inputs), np.array(aug_outputs)\n",
        "\n",
        "# Plot input and output fields side by side with color bars\n",
        "def plot_input_output(thk, slopesurx, slopesury, ubar, vbar):\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(12, 5))\n",
        "\n",
        "    # Plot inputs\n",
        "    im0 = axs[0, 0].imshow(thk, cmap='Blues')\n",
        "    axs[0, 0].set_title('Ice Thickness (thk)')\n",
        "    axs[0, 0].set_xlabel('x')\n",
        "    axs[0, 0].set_ylabel('y')\n",
        "    fig.colorbar(im0, ax=axs[0, 0], orientation='vertical')\n",
        "\n",
        "    im1 = axs[0, 1].imshow(slopesurx, cmap='RdBu')\n",
        "    axs[0, 1].set_title('Surface Slope x (slopesurx)')\n",
        "    axs[0, 1].set_xlabel('x')\n",
        "    axs[0, 1].set_ylabel('y')\n",
        "    fig.colorbar(im1, ax=axs[0, 1], orientation='vertical')\n",
        "\n",
        "    im2 = axs[0, 2].imshow(slopesury, cmap='RdBu')\n",
        "    axs[0, 2].set_title('Surface Slope y (slopesury)')\n",
        "    axs[0, 2].set_xlabel('x')\n",
        "    axs[0, 2].set_ylabel('y')\n",
        "    fig.colorbar(im2, ax=axs[0, 2], orientation='vertical')\n",
        "\n",
        "    # Plot outputs\n",
        "    im3 = axs[1, 0].imshow(ubar, cmap='viridis')\n",
        "    axs[1, 0].set_title('Velocity x (ubar)')\n",
        "    axs[1, 0].set_xlabel('x')\n",
        "    axs[1, 0].set_ylabel('y')\n",
        "    fig.colorbar(im3, ax=axs[1, 0], orientation='vertical')\n",
        "\n",
        "    im4 = axs[1, 1].imshow(vbar, cmap='viridis')\n",
        "    axs[1, 1].set_title('Velocity y (vbar)')\n",
        "    axs[1, 1].set_xlabel('x')\n",
        "    axs[1, 1].set_ylabel('y')\n",
        "    fig.colorbar(im4, ax=axs[1, 1], orientation='vertical')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_comparison(thk, slopesurx, slopesury, true_ubar, true_vbar, pred_ubar, pred_vbar, time_idx=0):\n",
        "    fig, axs = plt.subplots(3, 3, figsize=(12, 7))\n",
        "\n",
        "    # Plot inputs with individual color bars\n",
        "    im = axs[0, 0].imshow(thk[time_idx, :, :], cmap='Blues')\n",
        "    axs[0, 0].set_title('Ice Thickness (thk)')\n",
        "    axs[0, 0].set_xlabel('x')\n",
        "    axs[0, 0].set_ylabel('y')\n",
        "    fig.colorbar(im, ax=axs[0, 0])\n",
        "\n",
        "    im = axs[0, 1].imshow(slopesurx[time_idx, :, :], cmap='RdBu')\n",
        "    axs[0, 1].set_title('Surface Slope x (slopesurx)')\n",
        "    axs[0, 1].set_xlabel('x')\n",
        "    axs[0, 1].set_ylabel('y')\n",
        "    fig.colorbar(im, ax=axs[0, 1])\n",
        "\n",
        "    im = axs[0, 2].imshow(slopesury[time_idx, :, :], cmap='RdBu')\n",
        "    axs[0, 2].set_title('Surface Slope y (slopesury)')\n",
        "    axs[0, 2].set_xlabel('x')\n",
        "    axs[0, 2].set_ylabel('y')\n",
        "    fig.colorbar(im, ax=axs[0, 2])\n",
        "\n",
        "    # Determine the shared vmin and vmax for velocity comparisons\n",
        "    vmin_ubar = min(true_ubar.min(), pred_ubar.min())\n",
        "    vmax_ubar = max(true_ubar.max(), pred_ubar.max())\n",
        "    vmin_vbar = min(true_vbar.min(), pred_vbar.min())\n",
        "    vmax_vbar = max(true_vbar.max(), pred_vbar.max())\n",
        "\n",
        "    # Plot true outputs with shared color bars\n",
        "    im_ubar_true = axs[1, 0].imshow(true_ubar[time_idx, :, :], cmap='viridis', vmin=vmin_ubar, vmax=vmax_ubar)\n",
        "    axs[1, 0].set_title('True Velocity x (ubar)')\n",
        "    axs[1, 0].set_xlabel('x')\n",
        "    axs[1, 0].set_ylabel('y')\n",
        "    fig.colorbar(im_ubar_true, ax=axs[1, 0])\n",
        "\n",
        "    im_vbar_true = axs[2, 0].imshow(true_vbar[time_idx, :, :], cmap='viridis', vmin=vmin_vbar, vmax=vmax_vbar)\n",
        "    axs[2, 0].set_title('True Velocity y (vbar)')\n",
        "    axs[2, 0].set_xlabel('x')\n",
        "    axs[2, 0].set_ylabel('y')\n",
        "    fig.colorbar(im_vbar_true, ax=axs[2, 0])\n",
        "\n",
        "    # Plot predicted outputs with the same color scales\n",
        "    im_ubar_pred = axs[1, 1].imshow(pred_ubar[time_idx, :, :], cmap='viridis', vmin=vmin_ubar, vmax=vmax_ubar)\n",
        "    axs[1, 1].set_title('Predicted Velocity x (ubar)')\n",
        "    axs[1, 1].set_xlabel('x')\n",
        "    axs[1, 1].set_ylabel('y')\n",
        "    fig.colorbar(im_ubar_pred, ax=axs[1, 1])\n",
        "\n",
        "    im_vbar_pred = axs[2, 1].imshow(pred_vbar[time_idx, :, :], cmap='viridis', vmin=vmin_vbar, vmax=vmax_vbar)\n",
        "    axs[2, 1].set_title('Predicted Velocity y (vbar)')\n",
        "    axs[2, 1].set_xlabel('x')\n",
        "    axs[2, 1].set_ylabel('y')\n",
        "    fig.colorbar(im_vbar_pred, ax=axs[2, 1])\n",
        "\n",
        "    # Plot difference between true and predicted velocities\n",
        "    im_ubar_diff = axs[1, 2].imshow(true_ubar[time_idx, :, :] - pred_ubar[time_idx, :, :], cmap='RdBu')\n",
        "    axs[1, 2].set_title('Difference Velocity x')\n",
        "    axs[1, 2].set_xlabel('x')\n",
        "    axs[1, 2].set_ylabel('y')\n",
        "    fig.colorbar(im_ubar_diff, ax=axs[1, 2])\n",
        "\n",
        "    im_vbar_diff = axs[2, 2].imshow(true_vbar[time_idx, :, :] - pred_vbar[time_idx, :, :], cmap='RdBu')\n",
        "    axs[2, 2].set_title('Difference Velocity y')\n",
        "    axs[2, 2].set_xlabel('x')\n",
        "    axs[2, 2].set_ylabel('y')\n",
        "    fig.colorbar(im_vbar_diff, ax=axs[2, 2])\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT9gYaFZk7fn"
      },
      "source": [
        "##### Functions to prepare the data for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Yeb4rAk7fn"
      },
      "outputs": [],
      "source": [
        "def prepare_data (merged_data):\n",
        "    # Step 1: Extract the input and output fields\n",
        "    thk = merged_data['thk'].values           # Ice thickness\n",
        "    slopsurfx = merged_data['slopsurfx'].values # Surface slope in x\n",
        "    slopsurfy = merged_data['slopsurfy'].values # Surface slope in y\n",
        "    ubar = merged_data['ubar'].values         # Velocity x component\n",
        "    vbar = merged_data['vbar'].values         # Velocity y component\n",
        "    usurf = merged_data['usurf'].values\n",
        "\n",
        "    # Calculate 90th percentile scaling factors for training\n",
        "    scaling_factors = {\n",
        "        \"thk\": np.percentile(thk.max(axis=(1, 2)), 90),\n",
        "        \"slopsurfx\": np.percentile(slopsurfx.max(axis=(1, 2)), 90),\n",
        "        \"slopsurfy\": np.percentile(slopsurfy.max(axis=(1, 2)), 90),\n",
        "        \"ubar\": np.percentile(ubar.max(axis=(1, 2)), 90),\n",
        "        \"vbar\": np.percentile(vbar.max(axis=(1, 2)), 90)\n",
        "    }\n",
        "\n",
        "    # Step 2: Scale each field using the 90th percentile of its maximum\n",
        "    thk_scaled = scale_field(thk)\n",
        "    slopsurfx_scaled = scale_field(slopsurfx)\n",
        "    slopsurfy_scaled = scale_field(slopsurfy)\n",
        "    ubar_scaled = scale_field(ubar)\n",
        "    vbar_scaled = scale_field(vbar)\n",
        "    usurf_scaled = scale_field(usurf)\n",
        "\n",
        "    # Step 3: Stack inputs and outputs after scaling\n",
        "    inputs_scaled = np.stack([thk_scaled, slopsurfx_scaled, slopsurfy_scaled], axis=-1)  # Shape: (time, y, x, 3)\n",
        "    outputs_scaled = np.stack([ubar_scaled, vbar_scaled], axis=-1)  # Shape: (time, y, x, 2)\n",
        "\n",
        "    # Check shapes\n",
        "    print(f\"Inputs scaled shape: {inputs_scaled.shape}\")\n",
        "    print(f\"Outputs scaled shape: {outputs_scaled.shape}\")\n",
        "\n",
        "    # Visualize the data\n",
        "\n",
        "    time_idx=20\n",
        "    plot_input_output(thk[time_idx], slopsurfx[time_idx], slopsurfy[time_idx], ubar[time_idx], vbar[time_idx])\n",
        "\n",
        "    return inputs_scaled, outputs_scaled, scaling_factors\n",
        "\n",
        "def augment_data_for_training(inputs_scaled,outputs_scaled):\n",
        "    # Split index for 90-10 split\n",
        "    split_idx = int(0.9 * inputs_scaled.shape[0])\n",
        "\n",
        "    # Train-test split for inputs\n",
        "    X_train = inputs_scaled[:split_idx, :, :, :]\n",
        "    X_test = inputs_scaled[split_idx:, :, :, :]\n",
        "\n",
        "    # Train-test split for outputs\n",
        "    y_train = outputs_scaled[:split_idx, :, :, :]\n",
        "    y_test = outputs_scaled[split_idx:, :, :, :]\n",
        "\n",
        "    # Apply data augmentation\n",
        "    X_train_aug, y_train_aug = augment_data(X_train, y_train)\n",
        "    X_test_aug, y_test_aug = augment_data(X_test,y_test)\n",
        "    # Check shapes\n",
        "    print(f\"X_train shape: {X_train_aug.shape}\")\n",
        "    print(f\"y_train shape: {y_train_aug.shape}\")\n",
        "    print(f\"X_test shape: {X_test_aug.shape}\")\n",
        "    print(f\"y_test shape: {y_test_aug.shape}\")\n",
        "\n",
        "    return X_test_aug, X_train_aug, y_test_aug, y_train_aug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBBeu8t_k7fn"
      },
      "source": [
        "##### Start working on the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBygUqmxk7fn"
      },
      "outputs": [],
      "source": [
        "# Configuration as a dictionary\n",
        "config = {\n",
        "    \"nb_layers\": 4,               # Number of convolutional layers\n",
        "    \"nb_out_filter\": 32,           # Number of output filters for Conv2D\n",
        "    \"conv_ker_size\": 3,            # Convolution kernel size\n",
        "    \"activation\": \"relu\",          # Activation function: \"relu\" or \"lrelu\"\n",
        "    \"dropout_rate\": 0.1,           # Dropout rate\n",
        "    \"regularization\": 0.0001       # L2 regularization\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtp51LC6k7fn"
      },
      "outputs": [],
      "source": [
        "def build_cnn(nb_inputs, nb_outputs, config):\n",
        "    \"\"\"\n",
        "    Build a convolutional neural network (CNN) for glacier velocity field prediction.\n",
        "\n",
        "    Parameters:\n",
        "    - nb_inputs: Number of input channels (thk, slopsurfx, slopsurfy).\n",
        "    - nb_outputs: Number of output channels (ubar, vbar).\n",
        "    - config: Dictionary containing CNN configuration.\n",
        "\n",
        "    Returns:\n",
        "    - A compiled Keras model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input layer\n",
        "    inputs = tf.keras.layers.Input(shape=[None, None, nb_inputs])\n",
        "\n",
        "    conv = inputs\n",
        "\n",
        "    # Activation function choice\n",
        "    if config['activation'] == \"lrelu\":\n",
        "        activation = tf.keras.layers.LeakyReLU(alpha=0.01)\n",
        "    else:\n",
        "        activation = tf.keras.layers.ReLU()\n",
        "\n",
        "    # Stack convolutional layers\n",
        "    for i in range(config['nb_layers']):\n",
        "        conv = tf.keras.layers.Conv2D(\n",
        "            filters=config['nb_out_filter'],\n",
        "            kernel_size=(config['conv_ker_size'], config['conv_ker_size']),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(config['regularization']),\n",
        "            padding=\"same\"\n",
        "        )(conv)\n",
        "\n",
        "        conv = activation(conv)\n",
        "\n",
        "        conv = tf.keras.layers.Dropout(config['dropout_rate'])(conv)\n",
        "\n",
        "    # Output layer with nb_outputs channels (for ubar, vbar)\n",
        "    outputs = tf.keras.layers.Conv2D(\n",
        "        filters=nb_outputs,\n",
        "        kernel_size=(1, 1),\n",
        "        activation=None\n",
        "    )(conv)\n",
        "\n",
        "    # Return the complete model\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCGSF_Ask7fo",
        "outputId": "9505faca-922a-4496-e30a-d4352d668283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, None, None,   896         ['input_1[0][0]']                \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, None, None,   0           ['conv2d[0][0]',                 \n",
            "                                32)                               'conv2d_1[0][0]',               \n",
            "                                                                  'conv2d_2[0][0]',               \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, None,   0           ['re_lu[0][0]']                  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, None, None,   9248        ['dropout[0][0]']                \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, None, None,   0           ['re_lu[1][0]']                  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, None, None,   9248        ['dropout_1[0][0]']              \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, None, None,   0           ['re_lu[2][0]']                  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, None, None,   9248        ['dropout_2[0][0]']              \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, None, None,   0           ['re_lu[3][0]']                  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, None, None,   66          ['dropout_3[0][0]']              \n",
            "                                2)                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 28,706\n",
            "Trainable params: 28,706\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-13 10:08:16.232973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2024-10-13 10:08:16.233305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:16.233378: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:16.233409: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:16.233437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:16.347368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:16.347428: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2024-10-13 10:08:16.347433: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2024-10-13 10:08:16.347970: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Build and Compile the Model\n",
        "\n",
        "# Define the number of input channels (thk, slopsurfx, slopsurfy) and output channels (ubar, vbar)\n",
        "nb_inputs = 3  # thk, slopsurfx, slopsurfy\n",
        "nb_outputs = 2  # ubar, vbar\n",
        "\n",
        "# Build the CNN model\n",
        "model = build_cnn(nb_inputs, nb_outputs, config)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[\"mae\", \"mse\"])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PBDZCWZk7fo",
        "outputId": "421072ac-6850-4379-eb34-50c6eedc7889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs scaled shape: (101, 323, 497, 3)\n",
            "Outputs scaled shape: (101, 323, 497, 2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 11 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Retrieve the files from the cloud using Pooch.\\n\",\n",
        "data_url = 'https://unils-my.sharepoint.com/:u:/g/personal/kejdi_lleshi_unil_ch/EZVN0nazQYFMoFkONuM4788BfSsSe3xjB-jSXYW9JibtHw?download=1'\n",
        "hash = None\n",
        "file = pooch.retrieve(data_url, known_hash=hash)\n",
        "\n",
        "# Load and prepare the necessary dataset\n",
        "merged_data= load_from_pooch(file)\n",
        "\n",
        "# Visualize the I/O variables\n",
        "inputs_scaled, outputs_scaled, scaling_factors = prepare_data(merged_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Oegpsrk7fo",
        "outputId": "b473c5dc-3bd2-4178-eff1-8c684f367c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (270, 323, 497, 3)\n",
            "y_train shape: (270, 323, 497, 2)\n",
            "X_test shape: (33, 323, 497, 3)\n",
            "y_test shape: (33, 323, 497, 2)\n",
            "Epoch 1/100\n",
            "34/34 [==============================] - 28s 816ms/step - loss: 0.0137 - mae: 0.0315 - mse: 0.0060 - val_loss: 0.0099 - val_mae: 0.0279 - val_mse: 0.0045\n",
            "Epoch 2/100\n",
            "34/34 [==============================] - 28s 812ms/step - loss: 0.0093 - mae: 0.0281 - mse: 0.0051 - val_loss: 0.0074 - val_mae: 0.0266 - val_mse: 0.0041\n",
            "Epoch 3/100\n",
            "34/34 [==============================] - 28s 833ms/step - loss: 0.0075 - mae: 0.0264 - mse: 0.0047 - val_loss: 0.0061 - val_mae: 0.0231 - val_mse: 0.0036\n",
            "Epoch 4/100\n",
            "34/34 [==============================] - 30s 869ms/step - loss: 0.0064 - mae: 0.0236 - mse: 0.0042 - val_loss: 0.0055 - val_mae: 0.0223 - val_mse: 0.0035\n",
            "Epoch 5/100\n",
            "34/34 [==============================] - 29s 867ms/step - loss: 0.0059 - mae: 0.0222 - mse: 0.0040 - val_loss: 0.0048 - val_mae: 0.0192 - val_mse: 0.0030\n",
            "Epoch 6/100\n",
            "34/34 [==============================] - 29s 863ms/step - loss: 0.0055 - mae: 0.0213 - mse: 0.0038 - val_loss: 0.0045 - val_mae: 0.0184 - val_mse: 0.0029\n",
            "Epoch 7/100\n",
            "34/34 [==============================] - 29s 858ms/step - loss: 0.0052 - mae: 0.0206 - mse: 0.0037 - val_loss: 0.0045 - val_mae: 0.0209 - val_mse: 0.0030\n",
            "Epoch 8/100\n",
            "34/34 [==============================] - 29s 867ms/step - loss: 0.0050 - mae: 0.0202 - mse: 0.0036 - val_loss: 0.0041 - val_mae: 0.0180 - val_mse: 0.0027\n",
            "Epoch 9/100\n",
            "34/34 [==============================] - 29s 853ms/step - loss: 0.0049 - mae: 0.0200 - mse: 0.0035 - val_loss: 0.0040 - val_mae: 0.0174 - val_mse: 0.0026\n",
            "Epoch 10/100\n",
            "34/34 [==============================] - 30s 890ms/step - loss: 0.0047 - mae: 0.0199 - mse: 0.0034 - val_loss: 0.0039 - val_mae: 0.0178 - val_mse: 0.0026\n",
            "Epoch 11/100\n",
            "34/34 [==============================] - 30s 891ms/step - loss: 0.0048 - mae: 0.0198 - mse: 0.0035 - val_loss: 0.0040 - val_mae: 0.0188 - val_mse: 0.0027\n",
            "Epoch 12/100\n",
            "34/34 [==============================] - 30s 887ms/step - loss: 0.0046 - mae: 0.0201 - mse: 0.0034 - val_loss: 0.0037 - val_mae: 0.0168 - val_mse: 0.0025\n",
            "Epoch 13/100\n",
            "34/34 [==============================] - 30s 892ms/step - loss: 0.0043 - mae: 0.0188 - mse: 0.0031 - val_loss: 0.0035 - val_mae: 0.0168 - val_mse: 0.0023\n",
            "Epoch 14/100\n",
            "34/34 [==============================] - 30s 888ms/step - loss: 0.0043 - mae: 0.0186 - mse: 0.0030 - val_loss: 0.0038 - val_mae: 0.0173 - val_mse: 0.0026\n",
            "Epoch 15/100\n",
            "34/34 [==============================] - 30s 889ms/step - loss: 0.0044 - mae: 0.0189 - mse: 0.0032 - val_loss: 0.0039 - val_mae: 0.0205 - val_mse: 0.0027\n",
            "Epoch 16/100\n",
            "34/34 [==============================] - 30s 889ms/step - loss: 0.0042 - mae: 0.0185 - mse: 0.0030 - val_loss: 0.0038 - val_mae: 0.0182 - val_mse: 0.0026\n",
            "Epoch 17/100\n",
            "34/34 [==============================] - 27s 800ms/step - loss: 0.0043 - mae: 0.0191 - mse: 0.0031 - val_loss: 0.0035 - val_mae: 0.0173 - val_mse: 0.0023\n",
            "Epoch 18/100\n",
            "34/34 [==============================] - 25s 731ms/step - loss: 0.0041 - mae: 0.0183 - mse: 0.0029 - val_loss: 0.0036 - val_mae: 0.0184 - val_mse: 0.0024\n",
            "Epoch 19/100\n",
            "34/34 [==============================] - 25s 735ms/step - loss: 0.0039 - mae: 0.0178 - mse: 0.0028 - val_loss: 0.0032 - val_mae: 0.0152 - val_mse: 0.0020\n",
            "Epoch 20/100\n",
            "34/34 [==============================] - 25s 737ms/step - loss: 0.0041 - mae: 0.0181 - mse: 0.0029 - val_loss: 0.0034 - val_mae: 0.0177 - val_mse: 0.0023\n",
            "Epoch 21/100\n",
            "34/34 [==============================] - 25s 723ms/step - loss: 0.0039 - mae: 0.0179 - mse: 0.0027 - val_loss: 0.0034 - val_mae: 0.0164 - val_mse: 0.0022\n",
            "Epoch 22/100\n",
            "34/34 [==============================] - 25s 737ms/step - loss: 0.0038 - mae: 0.0173 - mse: 0.0026 - val_loss: 0.0032 - val_mae: 0.0151 - val_mse: 0.0021\n",
            "Epoch 23/100\n",
            "34/34 [==============================] - 25s 732ms/step - loss: 0.0038 - mae: 0.0175 - mse: 0.0026 - val_loss: 0.0031 - val_mae: 0.0144 - val_mse: 0.0020\n",
            "Epoch 24/100\n",
            "34/34 [==============================] - 25s 735ms/step - loss: 0.0039 - mae: 0.0175 - mse: 0.0027 - val_loss: 0.0032 - val_mae: 0.0155 - val_mse: 0.0020\n",
            "Epoch 25/100\n",
            "34/34 [==============================] - 25s 732ms/step - loss: 0.0036 - mae: 0.0164 - mse: 0.0024 - val_loss: 0.0031 - val_mae: 0.0155 - val_mse: 0.0019\n",
            "Epoch 26/100\n",
            "34/34 [==============================] - 25s 735ms/step - loss: 0.0036 - mae: 0.0167 - mse: 0.0024 - val_loss: 0.0030 - val_mae: 0.0140 - val_mse: 0.0019\n",
            "Epoch 27/100\n",
            "34/34 [==============================] - 25s 726ms/step - loss: 0.0036 - mae: 0.0167 - mse: 0.0025 - val_loss: 0.0029 - val_mae: 0.0143 - val_mse: 0.0018\n",
            "Epoch 28/100\n",
            "34/34 [==============================] - 25s 733ms/step - loss: 0.0034 - mae: 0.0158 - mse: 0.0023 - val_loss: 0.0029 - val_mae: 0.0136 - val_mse: 0.0017\n",
            "Epoch 29/100\n",
            "34/34 [==============================] - 25s 731ms/step - loss: 0.0036 - mae: 0.0163 - mse: 0.0024 - val_loss: 0.0031 - val_mae: 0.0151 - val_mse: 0.0020\n",
            "Epoch 30/100\n",
            "34/34 [==============================] - 25s 743ms/step - loss: 0.0034 - mae: 0.0160 - mse: 0.0023 - val_loss: 0.0029 - val_mae: 0.0149 - val_mse: 0.0018\n",
            "Epoch 31/100\n",
            "34/34 [==============================] - 25s 726ms/step - loss: 0.0035 - mae: 0.0161 - mse: 0.0024 - val_loss: 0.0028 - val_mae: 0.0134 - val_mse: 0.0017\n",
            "Epoch 32/100\n",
            "34/34 [==============================] - 25s 727ms/step - loss: 0.0033 - mae: 0.0153 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0136 - val_mse: 0.0016\n",
            "Epoch 33/100\n",
            "34/34 [==============================] - 25s 733ms/step - loss: 0.0032 - mae: 0.0151 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0134 - val_mse: 0.0017\n",
            "Epoch 34/100\n",
            "34/34 [==============================] - 25s 735ms/step - loss: 0.0032 - mae: 0.0151 - mse: 0.0021 - val_loss: 0.0026 - val_mae: 0.0141 - val_mse: 0.0015\n",
            "Epoch 35/100\n",
            "34/34 [==============================] - 25s 731ms/step - loss: 0.0032 - mae: 0.0151 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0135 - val_mse: 0.0017\n",
            "Epoch 36/100\n",
            "34/34 [==============================] - 25s 734ms/step - loss: 0.0032 - mae: 0.0149 - mse: 0.0020 - val_loss: 0.0027 - val_mae: 0.0140 - val_mse: 0.0016\n",
            "Epoch 37/100\n",
            "34/34 [==============================] - 25s 732ms/step - loss: 0.0032 - mae: 0.0150 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0133 - val_mse: 0.0015\n",
            "Epoch 38/100\n",
            "34/34 [==============================] - 25s 738ms/step - loss: 0.0033 - mae: 0.0157 - mse: 0.0022 - val_loss: 0.0029 - val_mae: 0.0134 - val_mse: 0.0018\n",
            "Epoch 39/100\n",
            "34/34 [==============================] - 25s 738ms/step - loss: 0.0031 - mae: 0.0149 - mse: 0.0020 - val_loss: 0.0025 - val_mae: 0.0125 - val_mse: 0.0014\n",
            "Epoch 40/100\n",
            "34/34 [==============================] - 25s 737ms/step - loss: 0.0031 - mae: 0.0145 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0138 - val_mse: 0.0015\n",
            "Epoch 41/100\n",
            "34/34 [==============================] - 27s 790ms/step - loss: 0.0030 - mae: 0.0145 - mse: 0.0019 - val_loss: 0.0025 - val_mae: 0.0122 - val_mse: 0.0014\n",
            "Epoch 42/100\n",
            "34/34 [==============================] - 30s 897ms/step - loss: 0.0030 - mae: 0.0144 - mse: 0.0019 - val_loss: 0.0027 - val_mae: 0.0139 - val_mse: 0.0016\n",
            "Epoch 43/100\n",
            "34/34 [==============================] - 29s 851ms/step - loss: 0.0032 - mae: 0.0152 - mse: 0.0021 - val_loss: 0.0030 - val_mae: 0.0139 - val_mse: 0.0019\n",
            "Epoch 44/100\n",
            "34/34 [==============================] - 29s 839ms/step - loss: 0.0030 - mae: 0.0145 - mse: 0.0019 - val_loss: 0.0025 - val_mae: 0.0128 - val_mse: 0.0014\n",
            "Epoch 45/100\n",
            "34/34 [==============================] - 29s 841ms/step - loss: 0.0030 - mae: 0.0144 - mse: 0.0019 - val_loss: 0.0025 - val_mae: 0.0126 - val_mse: 0.0014\n",
            "Epoch 46/100\n",
            "34/34 [==============================] - 29s 846ms/step - loss: 0.0029 - mae: 0.0141 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0118 - val_mse: 0.0014\n",
            "Epoch 47/100\n",
            "34/34 [==============================] - 28s 839ms/step - loss: 0.0028 - mae: 0.0138 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0115 - val_mse: 0.0013\n",
            "Epoch 48/100\n",
            "34/34 [==============================] - 29s 849ms/step - loss: 0.0028 - mae: 0.0138 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0116 - val_mse: 0.0013\n",
            "Epoch 49/100\n",
            "34/34 [==============================] - 29s 857ms/step - loss: 0.0028 - mae: 0.0138 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0118 - val_mse: 0.0013\n",
            "Epoch 50/100\n",
            "34/34 [==============================] - 32s 957ms/step - loss: 0.0028 - mae: 0.0137 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0128 - val_mse: 0.0014\n",
            "Epoch 51/100\n",
            "34/34 [==============================] - 30s 888ms/step - loss: 0.0028 - mae: 0.0137 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0124 - val_mse: 0.0013\n",
            "Epoch 52/100\n",
            "34/34 [==============================] - 29s 853ms/step - loss: 0.0030 - mae: 0.0149 - mse: 0.0020 - val_loss: 0.0024 - val_mae: 0.0122 - val_mse: 0.0013\n",
            "Epoch 53/100\n",
            "34/34 [==============================] - 29s 848ms/step - loss: 0.0029 - mae: 0.0141 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0119 - val_mse: 0.0014\n",
            "Epoch 54/100\n",
            "34/34 [==============================] - 29s 853ms/step - loss: 0.0029 - mae: 0.0139 - mse: 0.0018 - val_loss: 0.0023 - val_mae: 0.0115 - val_mse: 0.0013\n",
            "Epoch 55/100\n",
            "34/34 [==============================] - 29s 855ms/step - loss: 0.0029 - mae: 0.0141 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0124 - val_mse: 0.0014\n",
            "Epoch 56/100\n",
            "34/34 [==============================] - 29s 856ms/step - loss: 0.0028 - mae: 0.0137 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0117 - val_mse: 0.0012\n",
            "Epoch 57/100\n",
            "34/34 [==============================] - 29s 853ms/step - loss: 0.0027 - mae: 0.0130 - mse: 0.0016 - val_loss: 0.0022 - val_mae: 0.0115 - val_mse: 0.0012\n",
            "Epoch 58/100\n",
            "34/34 [==============================] - 29s 854ms/step - loss: 0.0028 - mae: 0.0140 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0119 - val_mse: 0.0013\n",
            "Epoch 59/100\n",
            "34/34 [==============================] - 28s 823ms/step - loss: 0.0027 - mae: 0.0133 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0108 - val_mse: 0.0012\n",
            "Epoch 60/100\n",
            "34/34 [==============================] - 25s 722ms/step - loss: 0.0026 - mae: 0.0129 - mse: 0.0016 - val_loss: 0.0022 - val_mae: 0.0115 - val_mse: 0.0011\n",
            "Epoch 61/100\n",
            "34/34 [==============================] - 25s 740ms/step - loss: 0.0026 - mae: 0.0129 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0113 - val_mse: 0.0012\n",
            "Epoch 62/100\n",
            "34/34 [==============================] - 25s 731ms/step - loss: 0.0027 - mae: 0.0131 - mse: 0.0016 - val_loss: 0.0022 - val_mae: 0.0116 - val_mse: 0.0012\n",
            "Epoch 63/100\n",
            "34/34 [==============================] - 25s 727ms/step - loss: 0.0027 - mae: 0.0134 - mse: 0.0017 - val_loss: 0.0024 - val_mae: 0.0133 - val_mse: 0.0014\n",
            "Epoch 64/100\n",
            "34/34 [==============================] - 25s 723ms/step - loss: 0.0027 - mae: 0.0133 - mse: 0.0016 - val_loss: 0.0023 - val_mae: 0.0114 - val_mse: 0.0013\n",
            "Epoch 65/100\n",
            "34/34 [==============================] - 25s 733ms/step - loss: 0.0028 - mae: 0.0143 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0135 - val_mse: 0.0013\n",
            "Epoch 66/100\n",
            "34/34 [==============================] - 25s 726ms/step - loss: 0.0028 - mae: 0.0145 - mse: 0.0018 - val_loss: 0.0023 - val_mae: 0.0118 - val_mse: 0.0012\n",
            "Epoch 67/100\n",
            "34/34 [==============================] - 25s 735ms/step - loss: 0.0028 - mae: 0.0141 - mse: 0.0018 - val_loss: 0.0025 - val_mae: 0.0143 - val_mse: 0.0015\n",
            "Epoch 68/100\n",
            "34/34 [==============================] - 25s 735ms/step - loss: 0.0029 - mae: 0.0144 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0135 - val_mse: 0.0014\n",
            "Epoch 69/100\n",
            "34/34 [==============================] - 25s 731ms/step - loss: 0.0026 - mae: 0.0129 - mse: 0.0016 - val_loss: 0.0024 - val_mae: 0.0126 - val_mse: 0.0014\n",
            "Epoch 70/100\n",
            "34/34 [==============================] - 25s 732ms/step - loss: 0.0027 - mae: 0.0136 - mse: 0.0017 - val_loss: 0.0023 - val_mae: 0.0113 - val_mse: 0.0012\n",
            "Epoch 71/100\n",
            "34/34 [==============================] - 25s 722ms/step - loss: 0.0026 - mae: 0.0128 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0107 - val_mse: 0.0011\n",
            "Epoch 72/100\n",
            "34/34 [==============================] - 25s 724ms/step - loss: 0.0026 - mae: 0.0126 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0109 - val_mse: 0.0011\n",
            "Epoch 73/100\n",
            "34/34 [==============================] - 24s 717ms/step - loss: 0.0026 - mae: 0.0127 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0104 - val_mse: 0.0011\n",
            "Epoch 74/100\n",
            "34/34 [==============================] - 24s 721ms/step - loss: 0.0028 - mae: 0.0142 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0122 - val_mse: 0.0013\n",
            "Epoch 75/100\n",
            "34/34 [==============================] - 24s 719ms/step - loss: 0.0027 - mae: 0.0139 - mse: 0.0017 - val_loss: 0.0022 - val_mae: 0.0111 - val_mse: 0.0012\n",
            "Epoch 76/100\n",
            "34/34 [==============================] - 25s 724ms/step - loss: 0.0025 - mae: 0.0126 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0111 - val_mse: 0.0011\n",
            "Epoch 77/100\n",
            "34/34 [==============================] - 25s 728ms/step - loss: 0.0025 - mae: 0.0126 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0108 - val_mse: 0.0011\n",
            "Epoch 78/100\n",
            "34/34 [==============================] - 24s 719ms/step - loss: 0.0025 - mae: 0.0126 - mse: 0.0015 - val_loss: 0.0022 - val_mae: 0.0119 - val_mse: 0.0012\n",
            "Epoch 79/100\n",
            "34/34 [==============================] - 25s 734ms/step - loss: 0.0026 - mae: 0.0130 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0106 - val_mse: 0.0011\n",
            "Epoch 80/100\n",
            "34/34 [==============================] - 25s 722ms/step - loss: 0.0024 - mae: 0.0123 - mse: 0.0014 - val_loss: 0.0021 - val_mae: 0.0100 - val_mse: 0.0010\n",
            "Epoch 81/100\n",
            "34/34 [==============================] - 25s 723ms/step - loss: 0.0025 - mae: 0.0124 - mse: 0.0015 - val_loss: 0.0020 - val_mae: 0.0105 - val_mse: 9.8833e-04\n",
            "Epoch 82/100\n",
            "34/34 [==============================] - 25s 722ms/step - loss: 0.0025 - mae: 0.0125 - mse: 0.0015 - val_loss: 0.0023 - val_mae: 0.0133 - val_mse: 0.0013\n",
            "Epoch 83/100\n",
            "34/34 [==============================] - 25s 720ms/step - loss: 0.0025 - mae: 0.0128 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0105 - val_mse: 0.0011\n",
            "Epoch 84/100\n",
            "34/34 [==============================] - 25s 725ms/step - loss: 0.0025 - mae: 0.0125 - mse: 0.0015 - val_loss: 0.0020 - val_mae: 0.0106 - val_mse: 9.8987e-04\n",
            "Epoch 85/100\n",
            "34/34 [==============================] - 24s 721ms/step - loss: 0.0025 - mae: 0.0125 - mse: 0.0014 - val_loss: 0.0021 - val_mae: 0.0119 - val_mse: 0.0011\n",
            "Epoch 86/100\n",
            "34/34 [==============================] - 25s 724ms/step - loss: 0.0025 - mae: 0.0127 - mse: 0.0015 - val_loss: 0.0022 - val_mae: 0.0125 - val_mse: 0.0012\n",
            "Epoch 87/100\n",
            "34/34 [==============================] - 25s 725ms/step - loss: 0.0026 - mae: 0.0134 - mse: 0.0016 - val_loss: 0.0020 - val_mae: 0.0108 - val_mse: 0.0010\n",
            "Epoch 88/100\n",
            "34/34 [==============================] - 24s 716ms/step - loss: 0.0025 - mae: 0.0125 - mse: 0.0015 - val_loss: 0.0020 - val_mae: 0.0101 - val_mse: 0.0010\n",
            "Epoch 89/100\n",
            "34/34 [==============================] - 25s 734ms/step - loss: 0.0024 - mae: 0.0122 - mse: 0.0014 - val_loss: 0.0020 - val_mae: 0.0101 - val_mse: 9.8987e-04\n",
            "Epoch 90/100\n",
            "34/34 [==============================] - 24s 722ms/step - loss: 0.0024 - mae: 0.0122 - mse: 0.0014 - val_loss: 0.0021 - val_mae: 0.0103 - val_mse: 0.0011\n",
            "Epoch 91/100\n",
            "34/34 [==============================] - 25s 730ms/step - loss: 0.0024 - mae: 0.0122 - mse: 0.0014 - val_loss: 0.0021 - val_mae: 0.0107 - val_mse: 0.0011\n",
            "Epoch 92/100\n",
            "34/34 [==============================] - 25s 732ms/step - loss: 0.0024 - mae: 0.0122 - mse: 0.0014 - val_loss: 0.0020 - val_mae: 0.0104 - val_mse: 0.0010\n",
            "Epoch 93/100\n",
            "34/34 [==============================] - 25s 729ms/step - loss: 0.0024 - mae: 0.0121 - mse: 0.0014 - val_loss: 0.0020 - val_mae: 0.0097 - val_mse: 9.7307e-04\n",
            "Epoch 94/100\n",
            "34/34 [==============================] - 25s 723ms/step - loss: 0.0024 - mae: 0.0119 - mse: 0.0014 - val_loss: 0.0019 - val_mae: 0.0098 - val_mse: 9.3450e-04\n",
            "Epoch 95/100\n",
            "34/34 [==============================] - 25s 726ms/step - loss: 0.0024 - mae: 0.0121 - mse: 0.0014 - val_loss: 0.0020 - val_mae: 0.0104 - val_mse: 9.6628e-04\n",
            "Epoch 96/100\n",
            "34/34 [==============================] - 25s 722ms/step - loss: 0.0027 - mae: 0.0137 - mse: 0.0017 - val_loss: 0.0021 - val_mae: 0.0118 - val_mse: 0.0011\n",
            "Epoch 97/100\n",
            "34/34 [==============================] - 25s 726ms/step - loss: 0.0024 - mae: 0.0121 - mse: 0.0014 - val_loss: 0.0019 - val_mae: 0.0096 - val_mse: 9.5879e-04\n",
            "Epoch 98/100\n",
            "34/34 [==============================] - 24s 719ms/step - loss: 0.0023 - mae: 0.0120 - mse: 0.0014 - val_loss: 0.0020 - val_mae: 0.0104 - val_mse: 0.0010\n",
            "Epoch 99/100\n",
            "34/34 [==============================] - 25s 728ms/step - loss: 0.0027 - mae: 0.0138 - mse: 0.0017 - val_loss: 0.0022 - val_mae: 0.0126 - val_mse: 0.0012\n",
            "Epoch 100/100\n",
            "34/34 [==============================] - 25s 731ms/step - loss: 0.0024 - mae: 0.0119 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0106 - val_mse: 0.0010\n"
          ]
        }
      ],
      "source": [
        "if train_mode:\n",
        "    # Prepare the data for training\n",
        "    X_test_aug, X_train_aug, y_test_aug, y_train_aug=augment_data_for_training(inputs_scaled,outputs_scaled)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_aug, y_train_aug, batch_size=8, epochs=100, validation_data=(X_test_aug, y_test_aug))\n",
        "    # Save the trained model to a file\n",
        "    model.save('model.keras')\n",
        "\n",
        "\n",
        "else :\n",
        "    model_url='https://unils-my.sharepoint.com/:u:/g/personal/kejdi_lleshi_unil_ch/EdNZk6-82ItCmR0oisE135YBTv-TLc7mwPDuU2gPrTojGA?download=1'\n",
        "    hash = None\n",
        "    file = pooch.retrieve(model_url, known_hash=hash)\n",
        "    model = tf.keras.models.load_model(file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qU-BQ_xk7fo",
        "outputId": "c58eb1bf-8987-43ad-baa9-f34d92ec048e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 22ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x700 with 18 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualise the performance of the trained model by making predictions\n",
        "\n",
        "predicted_outputs = model.predict(X_test_aug)\n",
        "# Separate predicted outputs into ubar and vbar components\n",
        "pred_ubar = predicted_outputs[..., 0]  # First channel is ubar\n",
        "pred_vbar = predicted_outputs[..., 1]  # Second channel is vbar\n",
        "# Call the plot function for a specific time index, e.g., 0\n",
        "time_idx = 2\n",
        "plot_comparison(X_test_aug[..., 0], X_test_aug[..., 1], X_test_aug[..., 2], y_test_aug[..., 0], y_test_aug[..., 1], pred_ubar, pred_vbar, time_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_lmp1fdk7fp",
        "outputId": "a4614eaa-90b1-4289-d746-98887207dd13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Plot the Learning curve\n",
        "\n",
        "if train_mode:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "\n",
        "    # Plot validation loss\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.title('Learning Curve (Loss vs. Epochs)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.savefig(\"Lr.png\")\n",
        "    # Show plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ubqTjL8k7fp"
      },
      "source": [
        "\n",
        "#### Implementatiing the hybrid model\n",
        "\n",
        "For the hybrid model, we will implement the mass conservation equation in a numerical shceme. The main difference with the first approach will be how we calculate **u**(x,y). The emulator we trained above, we plug it in the mass conservation equation to calculate the flux.\n",
        "Now we will need a couple of helper functions, `compute_divflux()` and `compute_gradient_tf()`, for each itteration of the numerical scheme.\n",
        "Remember that the velocity field is mainly dependent on the slope and the thckness of the glacier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McEJlNf5k7fp"
      },
      "source": [
        "##### Helping functions for the hybrid model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-U5xGWok7fp"
      },
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def compute_divflux(u, v, h, dx, dy):\n",
        "    \"\"\"\n",
        "    Upwind computation of the divergence of the flux: d(u h)/dx + d(v h)/dy\n",
        "\n",
        "    Parameters:\n",
        "    - u: x-component of velocity (2D tensor).\n",
        "    - v: y-component of velocity (2D tensor).\n",
        "    - h: ice thickness (2D tensor).\n",
        "    - dx: grid spacing in the x-direction (float).\n",
        "    - dy: grid spacing in the y-direction (float).\n",
        "\n",
        "    Returns:\n",
        "    - divflux: divergence of flux (2D tensor).\n",
        "    \"\"\"\n",
        "    # Compute u and v on the staggered grid\n",
        "    u = tf.concat([u[:, 0:1], 0.5 * (u[:, :-1] + u[:, 1:]), u[:, -1:]], 1)  # shape (ny, nx+1)\n",
        "    v = tf.concat([v[0:1, :], 0.5 * (v[:-1, :] + v[1:, :]), v[-1:, :]], 0)  # shape (ny+1, nx)\n",
        "\n",
        "    # Extend h with constant value at the domain boundaries\n",
        "    Hx = tf.pad(h, [[0, 0], [1, 1]], \"CONSTANT\")  # shape (ny, nx+2)\n",
        "    Hy = tf.pad(h, [[1, 1], [0, 0]], \"CONSTANT\")  # shape (ny+2, nx)\n",
        "\n",
        "    # Compute fluxes by selecting the upwind quantities\n",
        "    Qx = u * tf.where(u > 0, Hx[:, :-1], Hx[:, 1:])  # shape (ny, nx+1)\n",
        "    Qy = v * tf.where(v > 0, Hy[:-1, :], Hy[1:, :])  # shape (ny+1, nx)\n",
        "\n",
        "    # Compute the divergence, final shape is (ny, nx)\n",
        "    divflux = (Qx[:, 1:] - Qx[:, :-1]) / dx + (Qy[1:, :] - Qy[:-1, :]) / dy\n",
        "    return divflux\n",
        "\n",
        "\n",
        "@tf.function()\n",
        "def compute_gradient_tf(s, dx, dy):\n",
        "    \"\"\"\n",
        "    Compute spatial 2D gradient of a given field.\n",
        "\n",
        "    Parameters:\n",
        "    - s: surface elevation (2D tensor).\n",
        "    - dx: grid spacing in the x-direction (float).\n",
        "    - dy: grid spacing in the y-direction (float).\n",
        "\n",
        "    Returns:\n",
        "    - diffx: gradient in the x-direction (2D tensor).\n",
        "    - diffy: gradient in the y-direction (2D tensor).\n",
        "    \"\"\"\n",
        "    EX = tf.concat([1.5 * s[:, 0:1] - 0.5 * s[:, 1:2], 0.5 * s[:, :-1] + 0.5 * s[:, 1:], 1.5 * s[:, -1:] - 0.5 * s[:, -2:-1]], 1)\n",
        "    diffx = (EX[:, 1:] - EX[:, :-1]) / dx\n",
        "\n",
        "    EY = tf.concat([1.5 * s[0:1, :] - 0.5 * s[1:2, :], 0.5 * s[:-1, :] + 0.5 * s[1:, :], 1.5 * s[-1:, :] - 0.5 * s[-2:-1, :]], 0)\n",
        "    diffy = (EY[1:, :] - EY[:-1, :]) / dy\n",
        "\n",
        "    return diffx, diffy\n",
        "\n",
        "\n",
        "def apply_boundary_condition(H_ice, boundary_width=5):\n",
        "    \"\"\"\n",
        "    Apply boundary condition to the ice thickness field `H_ice`.\n",
        "    The ice thickness will linearly decrease to zero starting from `boundary_width` pixels away from the boundary.\n",
        "\n",
        "    Parameters:\n",
        "    - H_ice: 2D numpy array representing ice thickness.\n",
        "    - boundary_width: Number of pixels from the boundary where H_ice starts to decrease.\n",
        "\n",
        "    Returns:\n",
        "    - Modified H_ice with boundary condition applied.\n",
        "    \"\"\"\n",
        "    ny, nx = H_ice.shape  # Get the dimensions of the ice thickness field\n",
        "\n",
        "    # Create linear ramps\n",
        "    ramp = np.linspace(1, 0, boundary_width)  # Ramp that linearly decreases from 1 to 0\n",
        "\n",
        "    # Apply boundary condition to the left boundary\n",
        "    H_ice[:, :boundary_width] *= ramp[::-1]  # Decrease from boundary to 5 pixels inwards\n",
        "\n",
        "    # Apply boundary condition to the right boundary\n",
        "    H_ice[:, -boundary_width:] *= ramp  # Decrease from 5 pixels inwards to the boundary\n",
        "\n",
        "    # Apply boundary condition to the top boundary\n",
        "    H_ice[:boundary_width, :] *= ramp[::-1, np.newaxis]  # Decrease vertically from top boundary\n",
        "\n",
        "    # Apply boundary condition to the bottom boundary\n",
        "    H_ice[-boundary_width:, :] *= ramp[:, np.newaxis]  # Decrease vertically to bottom boundary\n",
        "\n",
        "    return H_ice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW7f5V5wk7fp"
      },
      "source": [
        "##### Run the hybrid model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYWJYqwuk7fp"
      },
      "outputs": [],
      "source": [
        "# Physical parameters\n",
        "Lx = 49700    # Domain length in x (m)\n",
        "Ly = 32300    # Domain length in y (m)\n",
        "ttot = 700   # Time limit (yr)\n",
        "grad_b = 0.001 # Mass balance gradient (no unit)\n",
        "b_max = 0.5   # Maximum precip (m/yr)\n",
        "Z_ELA = 3000  # Elevation of equilibrium line altitude (m)\n",
        "\n",
        "# Initialization & load data\n",
        "\n",
        "nout = 50  # Frequency of plotting\n",
        "dtmax = 1   # maximum time step\n",
        "cfl = 0.20\n",
        "dx = 100\n",
        "dy = 100  # Cell size in y\n",
        "nx=int(Lx/dx)\n",
        "ny=int(Ly/dy)\n",
        "x = np.linspace(0, Lx, nx)  # x-coordinates\n",
        "y = np.linspace(0, Ly, ny)  # y-coordinates\n",
        "\n",
        "H_ice = np.zeros((ny, nx))  # Initial ice thickness\n",
        "Z_surf = Z_topo + H_ice  # Initial ice surface\n",
        "# Compute gradients of surface elevation (slopes)\n",
        "slopsurfx, slopsurfy = compute_gradient_tf(Z_surf, dx, dx)\n",
        "time = tf.cast(0.0, tf.float32)  # Initial time as float32\n",
        "dt = tf.cast(dtmax, tf.float32)  # Cast dtmax to float32e\n",
        "it = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2hfy5ANk7fp",
        "outputId": "d3e2c10e-c517-419d-eb92-fb5090418237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 2200x800 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Ensure all arrays are float32\n",
        "H_ice = tf.cast(H_ice, tf.float32)\n",
        "slopsurfx = tf.cast(slopsurfx, tf.float32)\n",
        "slopsurfy = tf.cast(slopsurfy, tf.float32)\n",
        "Z_surf = tf.cast(Z_surf, tf.float32)\n",
        "Z_topo = tf.cast(Z_topo, tf.float32)\n",
        "\n",
        "#Fields need to be scaled\n",
        "\n",
        "# Loop\n",
        "while time < ttot:\n",
        "    # Update time\n",
        "    time += dt\n",
        "    it += 1\n",
        "\n",
        "    # Calculate H_avg, size (ny-1, nx-1)\n",
        "    H_avg = 0.25 * (H_ice[:-1, :-1] + H_ice[1:, 1:] + H_ice[:-1, 1:] + H_ice[1:, :-1])\n",
        "\n",
        "    # Scale the inputs with stored scaling factors\n",
        "    H_ice_scaled = H_ice / scaling_factors[\"thk\"]\n",
        "    slopsurfx_scaled = slopsurfx / scaling_factors[\"slopsurfx\"]\n",
        "    slopsurfy_scaled = slopsurfy / scaling_factors[\"slopsurfy\"]\n",
        "\n",
        "    # Combine scaled inputs\n",
        "    input_data_scaled = np.stack([H_ice_scaled, slopsurfx_scaled, slopsurfy_scaled], axis=-1)\n",
        "    input_data_scaled = np.expand_dims(input_data_scaled, axis=0)  # Add batch dimension\n",
        "    # Step 2: Use the trained model to predict ubar (x-velocity) and vbar (y-velocity)\n",
        "\n",
        "    ubar_vbar_pred = model.predict(input_data_scaled, verbose=0)\n",
        "    ubar = ubar_vbar_pred[0, :, :, 0] * scaling_factors[\"ubar\"] # x-component of velocity (ubar)\n",
        "    vbar = ubar_vbar_pred[0, :, :, 1] * scaling_factors[\"vbar\"] # y-component of velocity (vbar)\n",
        "\n",
        "    # Step 3: Compute maximum velocity for CFL condition\n",
        "    vel_max = max(\n",
        "        tf.math.reduce_max(tf.math.abs(ubar)),\n",
        "        tf.math.reduce_max(tf.math.abs(vbar)),\n",
        "    ).numpy()\n",
        "\n",
        "    # Step 4: Compute time step (CFL condition)\n",
        "    dt = tf.cast(tf.minimum(cfl * dx / vel_max, dtmax), tf.float32)\n",
        "\n",
        "    # Step 5: Update rule (diffusion): Compute the change in thickness (dH/dt)\n",
        "    dHdt = -compute_divflux(ubar, vbar, H_ice, dx, dx)\n",
        "\n",
        "    # Update ice thickness (ensure no negative values)\n",
        "    H_ice += dt * dHdt\n",
        "\n",
        "    # Define the SMB (Surface Mass Balance)\n",
        "    b = tf.minimum(grad_b * (Z_surf - Z_ELA), b_max)\n",
        "\n",
        "    # Update rule (mass balance)\n",
        "    H_ice += dt * b\n",
        "\n",
        "    # Update rule (positive thickness)\n",
        "    H_ice = np.maximum(H_ice, 0)\n",
        "\n",
        "    # Apply the boundary condition before the next iteration\n",
        "    H_ice = apply_boundary_condition(H_ice)\n",
        "\n",
        "    # Update surface topography\n",
        "    Z_surf = Z_topo + H_ice\n",
        "\n",
        "    # Compute gradients of surface elevation (slopes)\n",
        "    slopsurfx, slopsurfy = compute_gradient_tf(Z_surf, dx, dx)\n",
        "\n",
        "    # Update ELA after 500 years\n",
        "    if time > 500:\n",
        "        Z_ELA = 2700\n",
        "\n",
        "   # Display\n",
        "    if it % nout == 0:\n",
        "        clear_output(wait=True)  # Clear the previous output in the notebook\n",
        "\n",
        "        plt.figure(2, figsize=(11, 4), dpi=200)\n",
        "\n",
        "        # First subplot: Ice surface\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(Z_surf, extent=[0, Lx/1000, 0, Ly/1000], cmap='terrain', origin='lower')\n",
        "        plt.colorbar(label='Elevation (m)')\n",
        "        plt.title('Ice Surface at ' + str(int(time)) + ' y')\n",
        "        plt.xlabel('Distance, km')\n",
        "        plt.ylabel('Distance, km')\n",
        "\n",
        "        # Second subplot: Ice thickness\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(np.where(H_ice > 0, H_ice, np.nan), extent=[0, Lx/1000, 0, Ly/1000], cmap='jet', origin='lower')\n",
        "        plt.colorbar(label='Ice Thickness (m)')\n",
        "        plt.title('Ice Thickness at ' + str(int(time)) + ' y')\n",
        "        plt.xlabel('Distance, km')\n",
        "        plt.ylabel('Distance, km')\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6dPWi-Ok7fq"
      },
      "source": [
        "**Q: What are some dissadvantage of the hybrid models?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSdWOxxYk7fq"
      },
      "source": [
        "* <i> Border conditions are difficoult to implement.</i>\n",
        "* <i> The model is as good as our data is </i>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLyJQEc-k7fq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUpDcyBgk7fq"
      },
      "source": [
        "**Q: What are some other fields where we can use Hybrid Models?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKftPfhk7fq"
      },
      "source": [
        "* <i> 1 </i>\n",
        "* <i> 2 </i>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJWyrogfk7fq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
